{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfaa3110-1555-4515-8300-56aaebf4fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1.docx\n",
      "Processing file: 10.docx\n",
      "Processing file: 11.docx\n",
      "Processing file: 12.docx\n",
      "Processing file: 13.docx\n",
      "Processing file: 14.docx\n",
      "Processing file: 15.docx\n",
      "Processing file: 16.docx\n",
      "Processing file: 17.docx\n",
      "Processing file: 18.docx\n",
      "Processing file: 19.docx\n",
      "Processing file: 2.docx\n",
      "Processing file: 20.docx\n",
      "Processing file: 21.docx\n",
      "Processing file: 22.docx\n",
      "Processing file: 23.docx\n",
      "Processing file: 24.docx\n",
      "Processing file: 25.docx\n",
      "Processing file: 26.docx\n",
      "Processing file: 27.docx\n",
      "Processing file: 28.docx\n",
      "Processing file: 29.docx\n",
      "Processing file: 3.docx\n",
      "Processing file: 30.docx\n",
      "Processing file: 31.docx\n",
      "Processing file: 32.docx\n",
      "Processing file: 33.docx\n",
      "Processing file: 34.docx\n",
      "Processing file: 35.docx\n",
      "Processing file: 36.docx\n",
      "Processing file: 37.docx\n",
      "Processing file: 39.docx\n",
      "Processing file: 4.docx\n",
      "Processing file: 40.docx\n",
      "Processing file: 41.docx\n",
      "Processing file: 42.docx\n",
      "Processing file: 43.docx\n",
      "Processing file: 44.docx\n",
      "Processing file: 45.docx\n",
      "Processing file: 46.docx\n",
      "Processing file: 47.docx\n",
      "Processing file: 48.docx\n",
      "Processing file: 49.docx\n",
      "Processing file: 5.docx\n",
      "Processing file: 50.docx\n",
      "Processing file: 51.docx\n",
      "Processing file: 52.docx\n",
      "Processing file: 53.docx\n",
      "Processing file: 54.docx\n",
      "Processing file: 55.docx\n",
      "Processing file: 56.docx\n",
      "Processing file: 57.docx\n",
      "Processing file: 58.docx\n",
      "Processing file: 59.docx\n",
      "Processing file: 6.docx\n",
      "Processing file: 60.docx\n",
      "Processing file: 61.docx\n",
      "Processing file: 62.docx\n",
      "Processing file: 63.docx\n",
      "Processing file: 64.docx\n",
      "Processing file: 65.docx\n",
      "Processing file: 66.docx\n",
      "Processing file: 67.docx\n",
      "Processing file: 68.docx\n",
      "Processing file: 69.docx\n",
      "Processing file: 7.docx\n",
      "Processing file: 70.docx\n",
      "Processing file: 71.docx\n",
      "Processing file: 72.docx\n",
      "Processing file: 8.docx\n",
      "Processing file: 9.docx\n",
      "\n",
      "Initial Flight DataFrame shape: (51572, 33)\n",
      "Dropped columns with >50% missing values: ['arrival.terminal', 'arrival.baggage', 'codeshared.airline.name', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.flight.number', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'arrival.gate', 'departure.gate', 'arrival.actualTime', 'arrival.estimatedRunway', 'arrival.actualRunway']\n",
      "Converted 'departure.scheduledTime' to datetime.\n",
      "Converted 'departure.estimatedTime' to datetime.\n",
      "Converted 'departure.actualTime' to datetime.\n",
      "Converted 'departure.estimatedRunway' to datetime.\n",
      "Converted 'departure.actualRunway' to datetime.\n",
      "Converted 'arrival.scheduledTime' to datetime.\n",
      "Converted 'arrival.estimatedTime' to datetime.\n",
      "Imputed missing 'departure.estimatedTime' with median time 2024-01-04 06:00:00.\n",
      "Imputed missing 'arrival.estimatedTime' with median time 2024-01-13 15:19:00.\n",
      "Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\n",
      "Imputed missing values in categorical column 'departure.terminal' with 'Unknown'.\n",
      "Encoded 'status' column.\n",
      "One-Hot Encoded 'airline.name' with top 10 categories.\n",
      "No 'codeshared.airline.name' column found to encode.\n",
      "No numerical columns to apply VarianceThreshold.\n",
      "Calculated 'departure.delay_minutes'.\n",
      "Extracted temporal features.\n",
      "One-Hot Encoded 'departure.day_of_week'.\n",
      "Removed low variance features. Remaining features: 39\n",
      "Loaded weather data from 1.xlsx\n",
      "Loaded weather data from 2.xlsx\n",
      "Loaded weather data from 3.xlsx\n",
      "Loaded weather data from 4.xlsx\n",
      "Loaded weather data from 5.xlsx\n",
      "Loaded weather data from 6.xlsx\n",
      "Loaded weather data from 7.xlsx\n",
      "Loaded weather data from 8.xlsx\n",
      "Loaded weather data from 9.xlsx\n",
      "Loaded weather data from 10.xlsx\n",
      "Loaded weather data from 11.xlsx\n",
      "Loaded weather data from 12.xlsx\n",
      "Loaded weather data from 13.xlsx\n",
      "Combined weather DataFrame shape: (13, 7)\n",
      "Dropped 78 rows with invalid 'Date'.\n",
      "Reshaped weather DataFrame shape: (0, 1)\n",
      "Reshaping resulted in an empty DataFrame.\n",
      "Merged flight data with weather data. Merged DataFrame shape: (51572, 40)\n",
      "Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\n",
      "\n",
      "Final cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\n",
      "\n",
      "Final DataFrame shape: (51572, 40)\n",
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51572 entries, 0 to 51571\n",
      "Data columns (total 40 columns):\n",
      " #   Column                                   Non-Null Count  Dtype         \n",
      "---  ------                                   --------------  -----         \n",
      " 0   airline.iataCode                         51572 non-null  object        \n",
      " 1   airline.icaoCode                         51572 non-null  object        \n",
      " 2   airline_airblue                          51572 non-null  bool          \n",
      " 3   airline_airsial                          51572 non-null  bool          \n",
      " 4   airline_british airways                  51572 non-null  bool          \n",
      " 5   airline_emirates                         51572 non-null  bool          \n",
      " 6   airline_flyjinnah                        51572 non-null  bool          \n",
      " 7   airline_klm                              51572 non-null  bool          \n",
      " 8   airline_oman air                         51572 non-null  bool          \n",
      " 9   airline_pakistan international airlines  51572 non-null  bool          \n",
      " 10  airline_qatar airways                    51572 non-null  bool          \n",
      " 11  airline_serene air                       51572 non-null  bool          \n",
      " 12  arrival.estimatedTime                    51572 non-null  datetime64[ns]\n",
      " 13  arrival.iataCode                         51572 non-null  object        \n",
      " 14  arrival.icaoCode                         51572 non-null  object        \n",
      " 15  arrival.scheduledTime                    51572 non-null  datetime64[ns]\n",
      " 16  departure.actualRunway                   30990 non-null  datetime64[ns]\n",
      " 17  departure.actualTime                     51572 non-null  datetime64[ns]\n",
      " 18  departure.day_of_week_Monday             51572 non-null  bool          \n",
      " 19  departure.day_of_week_Saturday           51572 non-null  bool          \n",
      " 20  departure.day_of_week_Sunday             51572 non-null  bool          \n",
      " 21  departure.day_of_week_Thursday           51572 non-null  bool          \n",
      " 22  departure.day_of_week_Tuesday            51572 non-null  bool          \n",
      " 23  departure.day_of_week_Wednesday          51572 non-null  bool          \n",
      " 24  departure.estimatedRunway                30990 non-null  datetime64[ns]\n",
      " 25  departure.estimatedTime                  51572 non-null  datetime64[ns]\n",
      " 26  departure.hour_of_day                    51572 non-null  int32         \n",
      " 27  departure.iataCode                       51572 non-null  object        \n",
      " 28  departure.icaoCode                       51572 non-null  object        \n",
      " 29  departure.month                          51572 non-null  int32         \n",
      " 30  departure.scheduledTime                  51572 non-null  datetime64[ns]\n",
      " 31  departure.terminal                       51572 non-null  object        \n",
      " 32  flight.iataNumber                        51572 non-null  object        \n",
      " 33  flight.icaoNumber                        51572 non-null  object        \n",
      " 34  flight.number                            51572 non-null  object        \n",
      " 35  status                                   51572 non-null  object        \n",
      " 36  status_encoded                           51572 non-null  int32         \n",
      " 37  type                                     51572 non-null  object        \n",
      " 38  departure.delay_minutes                  51572 non-null  float64       \n",
      " 39  Departure Date                           51572 non-null  object        \n",
      "dtypes: bool(16), datetime64[ns](7), float64(1), int32(3), object(13)\n",
      "memory usage: 9.6+ MB\n",
      "None\n",
      "\n",
      "Missing Values After All Processing:\n",
      "airline.iataCode                               0\n",
      "airline.icaoCode                               0\n",
      "airline_airblue                                0\n",
      "airline_airsial                                0\n",
      "airline_british airways                        0\n",
      "airline_emirates                               0\n",
      "airline_flyjinnah                              0\n",
      "airline_klm                                    0\n",
      "airline_oman air                               0\n",
      "airline_pakistan international airlines        0\n",
      "airline_qatar airways                          0\n",
      "airline_serene air                             0\n",
      "arrival.estimatedTime                          0\n",
      "arrival.iataCode                               0\n",
      "arrival.icaoCode                               0\n",
      "arrival.scheduledTime                          0\n",
      "departure.actualRunway                     20582\n",
      "departure.actualTime                           0\n",
      "departure.day_of_week_Monday                   0\n",
      "departure.day_of_week_Saturday                 0\n",
      "departure.day_of_week_Sunday                   0\n",
      "departure.day_of_week_Thursday                 0\n",
      "departure.day_of_week_Tuesday                  0\n",
      "departure.day_of_week_Wednesday                0\n",
      "departure.estimatedRunway                  20582\n",
      "departure.estimatedTime                        0\n",
      "departure.hour_of_day                          0\n",
      "departure.iataCode                             0\n",
      "departure.icaoCode                             0\n",
      "departure.month                                0\n",
      "departure.scheduledTime                        0\n",
      "departure.terminal                             0\n",
      "flight.iataNumber                              0\n",
      "flight.icaoNumber                              0\n",
      "flight.number                                  0\n",
      "status                                         0\n",
      "status_encoded                                 0\n",
      "type                                           0\n",
      "departure.delay_minutes                        0\n",
      "Departure Date                                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "def extract_json_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Extracts JSON string from a .docx file.\n",
    "    Assumes that the JSON content is enclosed within [ ] brackets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    full_text = []\n",
    "    json_started = False\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        # Detect the start of JSON array\n",
    "        if text.startswith('['):\n",
    "            json_started = True\n",
    "\n",
    "        if json_started:\n",
    "            full_text.append(text)\n",
    "            # Detect the end of JSON array\n",
    "            if text.endswith(']'):\n",
    "                break\n",
    "\n",
    "    json_str = '\\n'.join(full_text)\n",
    "\n",
    "    # Optional: Clean up the JSON string using regex if necessary\n",
    "    # For example, remove unwanted characters or fix formatting issues\n",
    "    json_str = re.sub(r'(?<!\\\\)\"', r'\"', json_str)  # Replace unescaped quotes if necessary\n",
    "\n",
    "    return json_str\n",
    "\n",
    "def parse_json_to_dataframe(json_str):\n",
    "    \"\"\"\n",
    "    Parses a JSON string to a pandas DataFrame.\n",
    "    Handles both single JSON array and multiple JSON objects.\n",
    "    \"\"\"\n",
    "    if not json_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the JSON string as a list\n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list):\n",
    "            df = pd.json_normalize(data, sep='.')\n",
    "            return df\n",
    "        else:\n",
    "            # If not a list, wrap it into a list\n",
    "            df = pd.json_normalize([data], sep='.')\n",
    "            return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_docx_files(directory_path):\n",
    "    \"\"\"\n",
    "    Processes all .docx files in the specified directory.\n",
    "    Returns a combined pandas DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            json_str = extract_json_from_docx(file_path)\n",
    "            df = parse_json_to_dataframe(json_str)\n",
    "            if df is not None:\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                print(f\"Failed to parse JSON in file: {filename}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No DataFrames to concatenate.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def encode_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    if 'airline.name' in df.columns:\n",
    "        top_airlines = df['airline.name'].value_counts().nlargest(top_n).index\n",
    "        df['airline.name'] = df['airline.name'].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "        df = pd.get_dummies(df, columns=['airline.name'], prefix='airline', drop_first=True)\n",
    "        print(f\"One-Hot Encoded 'airline.name' with top {top_n} categories.\")\n",
    "    return df\n",
    "\n",
    "def encode_codeshare_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'codeshared.airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    # Adjust the column name based on actual DataFrame columns\n",
    "    possible_column_names = ['codeshared.airline.name', 'codeshare.airline.name', 'codeshare_airline.name']\n",
    "    column_found = False\n",
    "    for col in possible_column_names:\n",
    "        if col in df.columns:\n",
    "            top_airlines = df[col].value_counts().nlargest(top_n).index\n",
    "            df[col] = df[col].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "            df = pd.get_dummies(df, columns=[col], prefix='codeshare_airline', drop_first=True)\n",
    "            print(f\"One-Hot Encoded '{col}' with top {top_n} categories.\")\n",
    "            column_found = True\n",
    "            break\n",
    "    if not column_found:\n",
    "        print(\"No 'codeshared.airline.name' column found to encode.\")\n",
    "    return df\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Encode 'status' column (binary or multi-class classification)\n",
    "    if 'status' in df.columns:\n",
    "        df['status_encoded'] = le.fit_transform(df['status'])\n",
    "        print(\"Encoded 'status' column.\")\n",
    "\n",
    "    # One-Hot Encode 'departure.day_of_week' if exists\n",
    "    if 'departure.day_of_week' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # One-Hot Encode 'airline.name' with top categories\n",
    "    df = encode_airline_names(df, top_n=10)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_all_categorical_variables(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes all relevant categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    df = encode_categorical_variables(df)\n",
    "    df = encode_codeshare_airline_names(df, top_n=top_n)\n",
    "    return df\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=50):\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame where the percentage of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    cols_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped columns with >{threshold}% missing values: {cols_to_drop}\")\n",
    "    return df\n",
    "\n",
    "def impute_datetime_columns(df, datetime_cols):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime and imputes missing values based on a logical hierarchy.\n",
    "    \"\"\"\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "            print(f\"Converted '{col}' to datetime.\")\n",
    "\n",
    "    # Impute 'departure.estimatedTime' and 'arrival.estimatedTime' with median\n",
    "    for col in ['departure.estimatedTime', 'arrival.estimatedTime']:\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            median_time = df[col].median()\n",
    "            df[col].fillna(median_time, inplace=True)\n",
    "            print(f\"Imputed missing '{col}' with median time {median_time}.\")\n",
    "\n",
    "    # Now, impute 'departure.actualTime' with 'departure.estimatedTime'\n",
    "    if 'departure.actualTime' in df.columns and 'departure.estimatedTime' in df.columns:\n",
    "        df['departure.actualTime'].fillna(df['departure.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\")\n",
    "\n",
    "    # Similarly, impute 'arrival.actualTime' with 'arrival.estimatedTime'\n",
    "    if 'arrival.actualTime' in df.columns and 'arrival.estimatedTime' in df.columns:\n",
    "        df['arrival.actualTime'].fillna(df['arrival.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'arrival.actualTime' with 'arrival.estimatedTime'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_remaining_columns(df):\n",
    "    \"\"\"\n",
    "    Imputes remaining numerical columns with median and categorical columns with 'Unknown'.\n",
    "    \"\"\"\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_value = df[col].median()\n",
    "            df[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputed missing values in numerical column '{col}' with median value {median_value}.\")\n",
    "\n",
    "    # Impute categorical columns with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"Imputed missing values in categorical column '{col}' with 'Unknown'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_low_variance_features(df, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Removes numerical features with variance below the specified threshold.\n",
    "    Non-numerical columns are retained without modification.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - threshold (float): The variance threshold.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with low variance numerical features removed.\n",
    "    \"\"\"\n",
    "    # Select numerical columns (int64, float64, uint8)\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "\n",
    "    if not numeric_cols:\n",
    "        print(\"No numerical columns to apply VarianceThreshold.\")\n",
    "        return df\n",
    "\n",
    "    # Initialize VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "    # Fit the selector on numerical data\n",
    "    try:\n",
    "        selector.fit(df[numeric_cols])\n",
    "    except ValueError as e:\n",
    "        print(f\"VarianceThreshold failed: {e}\")\n",
    "        return df\n",
    "\n",
    "    # Get the columns to keep\n",
    "    features_to_keep = [col for col, keep in zip(numeric_cols, selector.get_support()) if keep]\n",
    "\n",
    "    # Retain selected numerical features and all non-numerical features\n",
    "    non_numeric_cols = df.columns.difference(numeric_cols)\n",
    "    df_filtered = pd.concat([df[non_numeric_cols], df[features_to_keep]], axis=1)\n",
    "\n",
    "    print(f\"Removed low variance features. Remaining features: {len(df_filtered.columns)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def load_weather_data(weather_directory, num_files=13):\n",
    "    \"\"\"\n",
    "    Loads and concatenates weather data from multiple Excel files.\n",
    "    \n",
    "    Parameters:\n",
    "    - weather_directory (str): Path to the directory containing weather Excel files.\n",
    "    - num_files (int): Number of Excel files to load (e.g., first 13 files).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined weather data.\n",
    "    \"\"\"\n",
    "    weather_dfs = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_name = f\"{i}.xlsx\"\n",
    "        file_path = os.path.join(weather_directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df_weather = pd.read_excel(file_path)\n",
    "                print(f\"Loaded weather data from {file_name}\")\n",
    "                weather_dfs.append(df_weather)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Weather file {file_name} does not exist in {weather_directory}. Skipping.\")\n",
    "    \n",
    "    if weather_dfs:\n",
    "        combined_weather_df = pd.concat(weather_dfs, ignore_index=True)\n",
    "        print(f\"Combined weather DataFrame shape: {combined_weather_df.shape}\")\n",
    "        return combined_weather_df\n",
    "    else:\n",
    "        print(\"No weather data files loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def reshape_weather_data(weather_df, year=2023):\n",
    "    \"\"\"\n",
    "    Reshapes the weather data from wide to long format.\n",
    "\n",
    "    Assumes that:\n",
    "    - The first column is 'Time' containing metric names (e.g., Max, Avg, Min)\n",
    "    - Subsequent columns are days (e.g., Jul 1, Jul 2, ..., Jul 31)\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data.\n",
    "    - year (int): Year for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Reshaped weather data with one row per day and separate columns for each metric.\n",
    "    \"\"\"\n",
    "    # Check if 'Time' column exists\n",
    "    if 'Time' not in weather_df.columns:\n",
    "        print(\"No 'Time' column found in weather data.\")\n",
    "        return weather_df\n",
    "\n",
    "    # Rename 'Time' to 'Metric' for clarity\n",
    "    weather_df.rename(columns={'Time': 'Metric'}, inplace=True)\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    weather_long_df = weather_df.melt(id_vars=['Metric'], var_name='Day', value_name='Value')\n",
    "\n",
    "    # Parse the 'Day' column to create a 'Date' column\n",
    "    # Assuming 'Day' is in the format 'Jul 1', 'Jul 2', etc.\n",
    "    weather_long_df['Date'] = pd.to_datetime(\n",
    "        weather_long_df['Day'] + f' {year}',\n",
    "        format='%b %d %Y',\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Drop rows with invalid dates\n",
    "    initial_shape = weather_long_df.shape\n",
    "    weather_long_df.dropna(subset=['Date'], inplace=True)\n",
    "    final_shape = weather_long_df.shape\n",
    "    print(f\"Dropped {initial_shape[0] - final_shape[0]} rows with invalid 'Date'.\")\n",
    "\n",
    "    # Pivot the DataFrame to have metrics as separate columns\n",
    "    weather_pivot = weather_long_df.pivot_table(\n",
    "        index='Date',\n",
    "        columns='Metric',\n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Rename 'Total' to 'Precipitation (in)' if applicable\n",
    "    if 'Total' in weather_pivot.columns:\n",
    "        weather_pivot.rename(columns={'Total': 'Precipitation (in)'}, inplace=True)\n",
    "\n",
    "    # Convert metric columns to numeric types\n",
    "    for col in weather_pivot.columns:\n",
    "        if col != 'Date':\n",
    "            weather_pivot[col] = pd.to_numeric(weather_pivot[col], errors='coerce')\n",
    "\n",
    "    print(f\"Reshaped weather DataFrame shape: {weather_pivot.shape}\")\n",
    "    return weather_pivot\n",
    "\n",
    "def preprocess_weather_data(weather_df, year=2023):\n",
    "    \"\"\"\n",
    "    Preprocesses the weather data DataFrame.\n",
    "\n",
    "    - Reshapes the data from wide to long format.\n",
    "    - Parses 'Date' column to datetime.\n",
    "    - Handles missing values if any.\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data.\n",
    "    - year (int): Year for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed weather data.\n",
    "    \"\"\"\n",
    "    # Reshape the weather data\n",
    "    weather_pivot = reshape_weather_data(weather_df, year=year)\n",
    "\n",
    "    # Verify the reshaped data\n",
    "    if weather_pivot.empty:\n",
    "        print(\"Reshaping resulted in an empty DataFrame.\")\n",
    "        return weather_pivot\n",
    "\n",
    "    return weather_pivot\n",
    "\n",
    "def merge_flight_weather(flight_df, weather_df):\n",
    "    \"\"\"\n",
    "    Merges flight data with weather data based on departure date.\n",
    "    \n",
    "    Parameters:\n",
    "    - flight_df (pd.DataFrame): Cleaned flight data with 'departure.scheduledTime'.\n",
    "    - weather_df (pd.DataFrame): Cleaned and reshaped weather data with 'Date'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'departure.scheduledTime' is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(flight_df['departure.scheduledTime']):\n",
    "        flight_df['departure.scheduledTime'] = pd.to_datetime(flight_df['departure.scheduledTime'], errors='coerce')\n",
    "        print(\"Converted 'departure.scheduledTime' to datetime.\")\n",
    "    \n",
    "    # Create a 'Departure Date' column (date only, no time)\n",
    "    flight_df['Departure Date'] = flight_df['departure.scheduledTime'].dt.date\n",
    "    weather_df['Date'] = weather_df['Date'].dt.date  # Ensure 'Date' is date only\n",
    "\n",
    "    # Merge on 'Departure Date' and 'Date'\n",
    "    merged_df = pd.merge(\n",
    "        flight_df,\n",
    "        weather_df,\n",
    "        how='left',\n",
    "        left_on='Departure Date',\n",
    "        right_on='Date',\n",
    "        suffixes=('', '_weather')\n",
    "    )\n",
    "\n",
    "    # Drop the redundant 'Date' column from weather data\n",
    "    merged_df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    print(f\"Merged flight data with weather data. Merged DataFrame shape: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "def encode_and_save_label_encoder(df, column, encoder_filename):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column using LabelEncoder and saves the encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the column.\n",
    "    - column (str): The name of the column to encode.\n",
    "    - encoder_filename (str): The filename to save the encoder.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the encoded column.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{column}_encoded'] = le.fit_transform(df[column])\n",
    "        joblib.dump(le, encoder_filename)\n",
    "        print(f\"Encoded '{column}' column and saved LabelEncoder as '{encoder_filename}'.\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Specify the directories containing your data\n",
    "    flight_data_directory = 'ML-Proj-Dataset/Train/'    # Replace with your actual flight data path\n",
    "    weather_data_directory = 'ML-Proj-Dataset/Weather/'         # Replace with your actual weather data path\n",
    "\n",
    "    # Check if the directories exist\n",
    "    if not os.path.isdir(flight_data_directory):\n",
    "        print(f\"The directory '{flight_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "    if not os.path.isdir(weather_data_directory):\n",
    "        print(f\"The directory '{weather_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Process the .docx flight data files and get the combined DataFrame\n",
    "    flight_df = process_docx_files(flight_data_directory)\n",
    "\n",
    "    if flight_df.empty:\n",
    "        print(\"No flight data to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nInitial Flight DataFrame shape: {flight_df.shape}\")\n",
    "\n",
    "    # Define datetime columns in flight data\n",
    "    flight_datetime_cols = [\n",
    "        'departure.scheduledTime',\n",
    "        'departure.estimatedTime',\n",
    "        'departure.actualTime',\n",
    "        'departure.estimatedRunway',\n",
    "        'departure.actualRunway',\n",
    "        'arrival.scheduledTime',\n",
    "        'arrival.estimatedTime',\n",
    "        'arrival.actualTime',\n",
    "        'arrival.estimatedRunway',\n",
    "        'arrival.actualRunway'\n",
    "    ]\n",
    "\n",
    "    # Drop columns with >50% missing values\n",
    "    flight_df = drop_high_missing_columns(flight_df, threshold=50)\n",
    "\n",
    "    # Convert and impute datetime columns\n",
    "    flight_df = impute_datetime_columns(flight_df, flight_datetime_cols)\n",
    "\n",
    "    # Impute remaining numerical and categorical columns\n",
    "    flight_df = impute_remaining_columns(flight_df)\n",
    "\n",
    "    # Encode categorical variables with optimized methods\n",
    "    flight_df = encode_all_categorical_variables(flight_df, top_n=10)\n",
    "\n",
    "    # Remove low variance numerical features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.01)  # Adjust threshold as needed\n",
    "\n",
    "    # Calculate departure delay if possible\n",
    "    if 'departure.actualTime' in flight_df.columns and 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.delay_minutes'] = (flight_df['departure.actualTime'] - flight_df['departure.scheduledTime']).dt.total_seconds() / 60\n",
    "        print(\"Calculated 'departure.delay_minutes'.\")\n",
    "\n",
    "    # Extract temporal features if needed\n",
    "    if 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.day_of_week'] = flight_df['departure.scheduledTime'].dt.day_name()\n",
    "        flight_df['departure.hour_of_day'] = flight_df['departure.scheduledTime'].dt.hour\n",
    "        flight_df['departure.month'] = flight_df['departure.scheduledTime'].dt.month\n",
    "        print(\"Extracted temporal features.\")\n",
    "\n",
    "        # One-Hot Encode temporal features\n",
    "        flight_df = pd.get_dummies(flight_df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # Remove low variance numerical features again after adding temporal features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.0)  # Remove features with zero variance\n",
    "\n",
    "    # Impute 'departure.delay_minutes' with median if there are still missing values\n",
    "    if 'departure.delay_minutes' in flight_df.columns and flight_df['departure.delay_minutes'].isnull().sum() > 0:\n",
    "        median_delay = flight_df['departure.delay_minutes'].median()\n",
    "        flight_df['departure.delay_minutes'].fillna(median_delay, inplace=True)\n",
    "        print(f\"Imputed missing 'departure.delay_minutes' with median value {median_delay}.\")\n",
    "\n",
    "    # Load and preprocess weather data\n",
    "    weather_df = load_weather_data(weather_data_directory, num_files=13)\n",
    "    if weather_df.empty:\n",
    "        print(\"No weather data available for merging.\")\n",
    "    else:\n",
    "        weather_df = preprocess_weather_data(weather_df, year=2023)  # Adjust year if necessary\n",
    "\n",
    "        # Merge flight data with weather data\n",
    "        flight_df = merge_flight_weather(flight_df, weather_df)\n",
    "\n",
    "        # Handle any remaining missing weather data by imputing with median\n",
    "        weather_metrics = ['Temperature (°F)', 'Dew Point (°F)', 'Humidity (%)', \n",
    "                           'Wind Speed (mph)', 'Pressure (in)', 'Precipitation (in)']\n",
    "        \n",
    "        for metric in weather_metrics:\n",
    "            if metric in flight_df.columns:\n",
    "                if flight_df[metric].isnull().sum() > 0:\n",
    "                    median_value = flight_df[metric].median()\n",
    "                    flight_df[metric].fillna(median_value, inplace=True)\n",
    "                    print(f\"Imputed missing '{metric}' with median value {median_value}.\")\n",
    "\n",
    "    # Save LabelEncoder for 'status'\n",
    "    if 'status_encoded' in flight_df.columns:\n",
    "        le_status = LabelEncoder()\n",
    "        le_status.fit(flight_df['status'])\n",
    "        joblib.dump(le_status, 'label_encoder_status.pkl')\n",
    "        print(\"Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\")\n",
    "\n",
    "    # Save the final cleaned and merged DataFrame\n",
    "    flight_df.to_csv('final_cleaned_flight_data_with_weather.csv', index=False)\n",
    "    print(\"\\nFinal cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\")\n",
    "\n",
    "    # Display final DataFrame information\n",
    "    print(\"\\nFinal DataFrame shape:\", flight_df.shape)\n",
    "    print(\"\\nFinal DataFrame Info:\")\n",
    "    print(flight_df.info())\n",
    "\n",
    "    print(\"\\nMissing Values After All Processing:\")\n",
    "    print(flight_df.isnull().sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60af8052-1faf-4a95-afad-d3305a12fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline.iataCode</th>\n",
       "      <th>airline.icaoCode</th>\n",
       "      <th>airline_airblue</th>\n",
       "      <th>airline_airsial</th>\n",
       "      <th>airline_british airways</th>\n",
       "      <th>airline_emirates</th>\n",
       "      <th>airline_flyjinnah</th>\n",
       "      <th>airline_klm</th>\n",
       "      <th>airline_oman air</th>\n",
       "      <th>airline_pakistan international airlines</th>\n",
       "      <th>...</th>\n",
       "      <th>departure.scheduledTime</th>\n",
       "      <th>departure.terminal</th>\n",
       "      <th>flight.iataNumber</th>\n",
       "      <th>flight.icaoNumber</th>\n",
       "      <th>flight.number</th>\n",
       "      <th>status</th>\n",
       "      <th>status_encoded</th>\n",
       "      <th>type</th>\n",
       "      <th>departure.delay_minutes</th>\n",
       "      <th>Departure Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sv</td>\n",
       "      <td>sva</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-20 20:50:00</td>\n",
       "      <td>m</td>\n",
       "      <td>sv737</td>\n",
       "      <td>sva737</td>\n",
       "      <td>737</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>2023-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9p</td>\n",
       "      <td>fjl</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-18 15:05:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9p843</td>\n",
       "      <td>fjl843</td>\n",
       "      <td>843</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9p</td>\n",
       "      <td>fjl</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-23 09:50:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9p841</td>\n",
       "      <td>fjl841</td>\n",
       "      <td>841</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>237370.0</td>\n",
       "      <td>2023-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pk</td>\n",
       "      <td>pia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-26 23:30:00</td>\n",
       "      <td>m</td>\n",
       "      <td>pk205</td>\n",
       "      <td>pia205</td>\n",
       "      <td>205</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2023-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>er</td>\n",
       "      <td>sep</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-20 11:35:00</td>\n",
       "      <td>m</td>\n",
       "      <td>er723</td>\n",
       "      <td>sep723</td>\n",
       "      <td>723</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2023-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51567</th>\n",
       "      <td>9p</td>\n",
       "      <td>fjl</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-16 09:50:00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9p841</td>\n",
       "      <td>fjl841</td>\n",
       "      <td>841</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2023-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51568</th>\n",
       "      <td>sv</td>\n",
       "      <td>sva</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-16 11:40:00</td>\n",
       "      <td>m</td>\n",
       "      <td>sv735</td>\n",
       "      <td>sva735</td>\n",
       "      <td>735</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2023-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51569</th>\n",
       "      <td>pk</td>\n",
       "      <td>pia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-27 10:50:00</td>\n",
       "      <td>m</td>\n",
       "      <td>pk203</td>\n",
       "      <td>pia203</td>\n",
       "      <td>203</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>225.0</td>\n",
       "      <td>2023-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51570</th>\n",
       "      <td>pk</td>\n",
       "      <td>pia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-19 02:00:00</td>\n",
       "      <td>m</td>\n",
       "      <td>pk898</td>\n",
       "      <td>pia898</td>\n",
       "      <td>898</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>540.0</td>\n",
       "      <td>2023-11-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51571</th>\n",
       "      <td>pk</td>\n",
       "      <td>pia</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-11-23 06:30:00</td>\n",
       "      <td>m</td>\n",
       "      <td>pk731</td>\n",
       "      <td>pia731</td>\n",
       "      <td>731</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>departure</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2023-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51572 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline.iataCode airline.icaoCode  airline_airblue  airline_airsial  \\\n",
       "0                   sv              sva            False            False   \n",
       "1                   9p              fjl            False            False   \n",
       "2                   9p              fjl            False            False   \n",
       "3                   pk              pia            False            False   \n",
       "4                   er              sep            False            False   \n",
       "...                ...              ...              ...              ...   \n",
       "51567               9p              fjl            False            False   \n",
       "51568               sv              sva            False            False   \n",
       "51569               pk              pia            False            False   \n",
       "51570               pk              pia            False            False   \n",
       "51571               pk              pia            False            False   \n",
       "\n",
       "       airline_british airways  airline_emirates  airline_flyjinnah  \\\n",
       "0                        False             False              False   \n",
       "1                        False             False               True   \n",
       "2                        False             False               True   \n",
       "3                        False             False              False   \n",
       "4                        False             False              False   \n",
       "...                        ...               ...                ...   \n",
       "51567                    False             False               True   \n",
       "51568                    False             False              False   \n",
       "51569                    False             False              False   \n",
       "51570                    False             False              False   \n",
       "51571                    False             False              False   \n",
       "\n",
       "       airline_klm  airline_oman air  airline_pakistan international airlines  \\\n",
       "0            False             False                                    False   \n",
       "1            False             False                                    False   \n",
       "2            False             False                                    False   \n",
       "3            False             False                                     True   \n",
       "4            False             False                                    False   \n",
       "...            ...               ...                                      ...   \n",
       "51567        False             False                                    False   \n",
       "51568        False             False                                    False   \n",
       "51569        False             False                                     True   \n",
       "51570        False             False                                     True   \n",
       "51571        False             False                                     True   \n",
       "\n",
       "       ...  departure.scheduledTime  departure.terminal flight.iataNumber  \\\n",
       "0      ...      2023-07-20 20:50:00                   m             sv737   \n",
       "1      ...      2023-07-18 15:05:00             Unknown             9p843   \n",
       "2      ...      2023-07-23 09:50:00             Unknown             9p841   \n",
       "3      ...      2023-07-26 23:30:00                   m             pk205   \n",
       "4      ...      2023-07-20 11:35:00                   m             er723   \n",
       "...    ...                      ...                 ...               ...   \n",
       "51567  ...      2023-11-16 09:50:00             Unknown             9p841   \n",
       "51568  ...      2023-11-16 11:40:00                   m             sv735   \n",
       "51569  ...      2023-11-27 10:50:00                   m             pk203   \n",
       "51570  ...      2023-11-19 02:00:00                   m             pk898   \n",
       "51571  ...      2023-11-23 06:30:00                   m             pk731   \n",
       "\n",
       "      flight.icaoNumber flight.number  status status_encoded       type  \\\n",
       "0                sva737           737  active              0  departure   \n",
       "1                fjl843           843  active              0  departure   \n",
       "2                fjl841           841  active              0  departure   \n",
       "3                pia205           205  active              0  departure   \n",
       "4                sep723           723  active              0  departure   \n",
       "...                 ...           ...     ...            ...        ...   \n",
       "51567            fjl841           841  active              0  departure   \n",
       "51568            sva735           735  active              0  departure   \n",
       "51569            pia203           203  active              0  departure   \n",
       "51570            pia898           898  active              0  departure   \n",
       "51571            pia731           731  active              0  departure   \n",
       "\n",
       "       departure.delay_minutes  Departure Date  \n",
       "0                        -35.0      2023-07-20  \n",
       "1                          0.0      2023-07-18  \n",
       "2                     237370.0      2023-07-23  \n",
       "3                         21.0      2023-07-26  \n",
       "4                        340.0      2023-07-20  \n",
       "...                        ...             ...  \n",
       "51567                     13.0      2023-11-16  \n",
       "51568                      8.0      2023-11-16  \n",
       "51569                    225.0      2023-11-27  \n",
       "51570                    540.0      2023-11-19  \n",
       "51571                     17.0      2023-11-23  \n",
       "\n",
       "[51572 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('final_cleaned_flight_data_with_weather.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a850622a-0e51-4d7a-9a38-435189f4bd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['airline.iataCode', 'airline.icaoCode', 'airline_airblue',\n",
       "       'airline_airsial', 'airline_british airways', 'airline_emirates',\n",
       "       'airline_flyjinnah', 'airline_klm', 'airline_oman air',\n",
       "       'airline_pakistan international airlines', 'airline_qatar airways',\n",
       "       'airline_serene air', 'arrival.estimatedTime', 'arrival.iataCode',\n",
       "       'arrival.icaoCode', 'arrival.scheduledTime', 'departure.actualRunway',\n",
       "       'departure.actualTime', 'departure.day_of_week_Monday',\n",
       "       'departure.day_of_week_Saturday', 'departure.day_of_week_Sunday',\n",
       "       'departure.day_of_week_Thursday', 'departure.day_of_week_Tuesday',\n",
       "       'departure.day_of_week_Wednesday', 'departure.estimatedRunway',\n",
       "       'departure.estimatedTime', 'departure.hour_of_day',\n",
       "       'departure.iataCode', 'departure.icaoCode', 'departure.month',\n",
       "       'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber',\n",
       "       'flight.icaoNumber', 'flight.number', 'status', 'status_encoded',\n",
       "       'type', 'departure.delay_minutes', 'Departure Date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8039b2-c6a6-4f8a-8742-2305ce93920a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1.docx\n",
      "Processing file: 10.docx\n",
      "Processing file: 11.docx\n",
      "Processing file: 12.docx\n",
      "Processing file: 13.docx\n",
      "Processing file: 14.docx\n",
      "Processing file: 15.docx\n",
      "Processing file: 16.docx\n",
      "Processing file: 17.docx\n",
      "Processing file: 18.docx\n",
      "Processing file: 19.docx\n",
      "Processing file: 2.docx\n",
      "Processing file: 20.docx\n",
      "Processing file: 21.docx\n",
      "Processing file: 22.docx\n",
      "Processing file: 23.docx\n",
      "Processing file: 24.docx\n",
      "Processing file: 25.docx\n",
      "Processing file: 26.docx\n",
      "Processing file: 27.docx\n",
      "Processing file: 28.docx\n",
      "Processing file: 29.docx\n",
      "Processing file: 3.docx\n",
      "Processing file: 30.docx\n",
      "Processing file: 31.docx\n",
      "Processing file: 32.docx\n",
      "Processing file: 33.docx\n",
      "Processing file: 34.docx\n",
      "Processing file: 35.docx\n",
      "Processing file: 36.docx\n",
      "Processing file: 37.docx\n",
      "Processing file: 39.docx\n",
      "Processing file: 4.docx\n",
      "Processing file: 40.docx\n",
      "Processing file: 41.docx\n",
      "Processing file: 42.docx\n",
      "Processing file: 43.docx\n",
      "Processing file: 44.docx\n",
      "Processing file: 45.docx\n",
      "Processing file: 46.docx\n",
      "Processing file: 47.docx\n",
      "Processing file: 48.docx\n",
      "Processing file: 49.docx\n",
      "Processing file: 5.docx\n",
      "Processing file: 50.docx\n",
      "Processing file: 51.docx\n",
      "Processing file: 52.docx\n",
      "Processing file: 53.docx\n",
      "Processing file: 54.docx\n",
      "Processing file: 55.docx\n",
      "Processing file: 56.docx\n",
      "Processing file: 57.docx\n",
      "Processing file: 58.docx\n",
      "Processing file: 59.docx\n",
      "Processing file: 6.docx\n",
      "Processing file: 60.docx\n",
      "Processing file: 61.docx\n",
      "Processing file: 62.docx\n",
      "Processing file: 63.docx\n",
      "Processing file: 64.docx\n",
      "Processing file: 65.docx\n",
      "Processing file: 66.docx\n",
      "Processing file: 67.docx\n",
      "Processing file: 68.docx\n",
      "Processing file: 69.docx\n",
      "Processing file: 7.docx\n",
      "Processing file: 70.docx\n",
      "Processing file: 71.docx\n",
      "Processing file: 72.docx\n",
      "Processing file: 8.docx\n",
      "Processing file: 9.docx\n",
      "\n",
      "Initial Flight DataFrame shape: (51572, 33)\n",
      "Dropped columns with >50% missing values: ['arrival.terminal', 'arrival.baggage', 'codeshared.airline.name', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.flight.number', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'arrival.gate', 'departure.gate', 'arrival.actualTime', 'arrival.estimatedRunway', 'arrival.actualRunway']\n",
      "Converted 'departure.scheduledTime' to datetime.\n",
      "Converted 'departure.estimatedTime' to datetime.\n",
      "Converted 'departure.actualTime' to datetime.\n",
      "Converted 'departure.estimatedRunway' to datetime.\n",
      "Converted 'departure.actualRunway' to datetime.\n",
      "Converted 'arrival.scheduledTime' to datetime.\n",
      "Converted 'arrival.estimatedTime' to datetime.\n",
      "Imputed missing 'departure.estimatedTime' with median time 2024-01-04 06:00:00.\n",
      "Imputed missing 'arrival.estimatedTime' with median time 2024-01-13 15:19:00.\n",
      "Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\n",
      "Imputed missing values in categorical column 'departure.terminal' with 'Unknown'.\n",
      "Encoded 'status' column.\n",
      "One-Hot Encoded 'airline.name' with top 10 categories.\n",
      "No 'codeshared.airline.name' column found to encode.\n",
      "No numerical columns to apply VarianceThreshold.\n",
      "Calculated 'departure.delay_minutes'.\n",
      "Extracted temporal features.\n",
      "One-Hot Encoded 'departure.day_of_week'.\n",
      "Removed low variance features. Remaining features: 39\n",
      "Loaded weather data from 1.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 1.xlsx is empty. Skipping.\n",
      "Loaded weather data from 2.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 2.xlsx is empty. Skipping.\n",
      "Loaded weather data from 3.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 3.xlsx is empty. Skipping.\n",
      "Loaded weather data from 4.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 4.xlsx is empty. Skipping.\n",
      "Loaded weather data from 5.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 5.xlsx is empty. Skipping.\n",
      "Loaded weather data from 6.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 6.xlsx is empty. Skipping.\n",
      "Loaded weather data from 7.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 7.xlsx is empty. Skipping.\n",
      "Loaded weather data from 8.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 8.xlsx is empty. Skipping.\n",
      "Loaded weather data from 9.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 9.xlsx is empty. Skipping.\n",
      "Loaded weather data from 10.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 10.xlsx is empty. Skipping.\n",
      "Loaded weather data from 11.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 11.xlsx is empty. Skipping.\n",
      "Loaded weather data from 12.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 12.xlsx is empty. Skipping.\n",
      "Loaded weather data from 13.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 13.xlsx is empty. Skipping.\n",
      "No weather data files loaded or reshaped successfully.\n",
      "No weather data available for merging.\n",
      "Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\n",
      "\n",
      "Final cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\n",
      "\n",
      "Final DataFrame shape: (51572, 39)\n",
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51572 entries, 0 to 51571\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                   Non-Null Count  Dtype         \n",
      "---  ------                                   --------------  -----         \n",
      " 0   airline.iataCode                         51572 non-null  object        \n",
      " 1   airline.icaoCode                         51572 non-null  object        \n",
      " 2   airline_airblue                          51572 non-null  bool          \n",
      " 3   airline_airsial                          51572 non-null  bool          \n",
      " 4   airline_british airways                  51572 non-null  bool          \n",
      " 5   airline_emirates                         51572 non-null  bool          \n",
      " 6   airline_flyjinnah                        51572 non-null  bool          \n",
      " 7   airline_klm                              51572 non-null  bool          \n",
      " 8   airline_oman air                         51572 non-null  bool          \n",
      " 9   airline_pakistan international airlines  51572 non-null  bool          \n",
      " 10  airline_qatar airways                    51572 non-null  bool          \n",
      " 11  airline_serene air                       51572 non-null  bool          \n",
      " 12  arrival.estimatedTime                    51572 non-null  datetime64[ns]\n",
      " 13  arrival.iataCode                         51572 non-null  object        \n",
      " 14  arrival.icaoCode                         51572 non-null  object        \n",
      " 15  arrival.scheduledTime                    51572 non-null  datetime64[ns]\n",
      " 16  departure.actualRunway                   30990 non-null  datetime64[ns]\n",
      " 17  departure.actualTime                     51572 non-null  datetime64[ns]\n",
      " 18  departure.day_of_week_Monday             51572 non-null  bool          \n",
      " 19  departure.day_of_week_Saturday           51572 non-null  bool          \n",
      " 20  departure.day_of_week_Sunday             51572 non-null  bool          \n",
      " 21  departure.day_of_week_Thursday           51572 non-null  bool          \n",
      " 22  departure.day_of_week_Tuesday            51572 non-null  bool          \n",
      " 23  departure.day_of_week_Wednesday          51572 non-null  bool          \n",
      " 24  departure.estimatedRunway                30990 non-null  datetime64[ns]\n",
      " 25  departure.estimatedTime                  51572 non-null  datetime64[ns]\n",
      " 26  departure.hour_of_day                    51572 non-null  int32         \n",
      " 27  departure.iataCode                       51572 non-null  object        \n",
      " 28  departure.icaoCode                       51572 non-null  object        \n",
      " 29  departure.month                          51572 non-null  int32         \n",
      " 30  departure.scheduledTime                  51572 non-null  datetime64[ns]\n",
      " 31  departure.terminal                       51572 non-null  object        \n",
      " 32  flight.iataNumber                        51572 non-null  object        \n",
      " 33  flight.icaoNumber                        51572 non-null  object        \n",
      " 34  flight.number                            51572 non-null  object        \n",
      " 35  status                                   51572 non-null  object        \n",
      " 36  status_encoded                           51572 non-null  int32         \n",
      " 37  type                                     51572 non-null  object        \n",
      " 38  departure.delay_minutes                  51572 non-null  float64       \n",
      "dtypes: bool(16), datetime64[ns](7), float64(1), int32(3), object(12)\n",
      "memory usage: 9.2+ MB\n",
      "None\n",
      "\n",
      "Missing Values After All Processing:\n",
      "airline.iataCode                               0\n",
      "airline.icaoCode                               0\n",
      "airline_airblue                                0\n",
      "airline_airsial                                0\n",
      "airline_british airways                        0\n",
      "airline_emirates                               0\n",
      "airline_flyjinnah                              0\n",
      "airline_klm                                    0\n",
      "airline_oman air                               0\n",
      "airline_pakistan international airlines        0\n",
      "airline_qatar airways                          0\n",
      "airline_serene air                             0\n",
      "arrival.estimatedTime                          0\n",
      "arrival.iataCode                               0\n",
      "arrival.icaoCode                               0\n",
      "arrival.scheduledTime                          0\n",
      "departure.actualRunway                     20582\n",
      "departure.actualTime                           0\n",
      "departure.day_of_week_Monday                   0\n",
      "departure.day_of_week_Saturday                 0\n",
      "departure.day_of_week_Sunday                   0\n",
      "departure.day_of_week_Thursday                 0\n",
      "departure.day_of_week_Tuesday                  0\n",
      "departure.day_of_week_Wednesday                0\n",
      "departure.estimatedRunway                  20582\n",
      "departure.estimatedTime                        0\n",
      "departure.hour_of_day                          0\n",
      "departure.iataCode                             0\n",
      "departure.icaoCode                             0\n",
      "departure.month                                0\n",
      "departure.scheduledTime                        0\n",
      "departure.terminal                             0\n",
      "flight.iataNumber                              0\n",
      "flight.icaoNumber                              0\n",
      "flight.number                                  0\n",
      "status                                         0\n",
      "status_encoded                                 0\n",
      "type                                           0\n",
      "departure.delay_minutes                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "def extract_json_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Extracts JSON string from a .docx file.\n",
    "    Assumes that the JSON content is enclosed within [ ] brackets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    full_text = []\n",
    "    json_started = False\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        # Detect the start of JSON array\n",
    "        if text.startswith('['):\n",
    "            json_started = True\n",
    "\n",
    "        if json_started:\n",
    "            full_text.append(text)\n",
    "            # Detect the end of JSON array\n",
    "            if text.endswith(']'):\n",
    "                break\n",
    "\n",
    "    json_str = '\\n'.join(full_text)\n",
    "\n",
    "    # Optional: Clean up the JSON string using regex if necessary\n",
    "    # For example, remove unwanted characters or fix formatting issues\n",
    "    json_str = re.sub(r'(?<!\\\\)\"', r'\"', json_str)  # Replace unescaped quotes if necessary\n",
    "\n",
    "    return json_str\n",
    "\n",
    "def parse_json_to_dataframe(json_str):\n",
    "    \"\"\"\n",
    "    Parses a JSON string to a pandas DataFrame.\n",
    "    Handles both single JSON array and multiple JSON objects.\n",
    "    \"\"\"\n",
    "    if not json_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the JSON string as a list\n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list):\n",
    "            df = pd.json_normalize(data, sep='.')\n",
    "            return df\n",
    "        else:\n",
    "            # If not a list, wrap it into a list\n",
    "            df = pd.json_normalize([data], sep='.')\n",
    "            return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_docx_files(directory_path):\n",
    "    \"\"\"\n",
    "    Processes all .docx files in the specified directory.\n",
    "    Returns a combined pandas DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            json_str = extract_json_from_docx(file_path)\n",
    "            df = parse_json_to_dataframe(json_str)\n",
    "            if df is not None:\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                print(f\"Failed to parse JSON in file: {filename}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No DataFrames to concatenate.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def encode_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    if 'airline.name' in df.columns:\n",
    "        top_airlines = df['airline.name'].value_counts().nlargest(top_n).index\n",
    "        df['airline.name'] = df['airline.name'].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "        df = pd.get_dummies(df, columns=['airline.name'], prefix='airline', drop_first=True)\n",
    "        print(f\"One-Hot Encoded 'airline.name' with top {top_n} categories.\")\n",
    "    return df\n",
    "\n",
    "def encode_codeshare_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'codeshared.airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    # Adjust the column name based on actual DataFrame columns\n",
    "    possible_column_names = ['codeshared.airline.name', 'codeshare.airline.name', 'codeshare_airline.name']\n",
    "    column_found = False\n",
    "    for col in possible_column_names:\n",
    "        if col in df.columns:\n",
    "            top_airlines = df[col].value_counts().nlargest(top_n).index\n",
    "            df[col] = df[col].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "            df = pd.get_dummies(df, columns=[col], prefix='codeshare_airline', drop_first=True)\n",
    "            print(f\"One-Hot Encoded '{col}' with top {top_n} categories.\")\n",
    "            column_found = True\n",
    "            break\n",
    "    if not column_found:\n",
    "        print(\"No 'codeshared.airline.name' column found to encode.\")\n",
    "    return df\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Encode 'status' column (binary or multi-class classification)\n",
    "    if 'status' in df.columns:\n",
    "        df['status_encoded'] = le.fit_transform(df['status'])\n",
    "        print(\"Encoded 'status' column.\")\n",
    "\n",
    "    # One-Hot Encode 'departure.day_of_week' if exists\n",
    "    if 'departure.day_of_week' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # One-Hot Encode 'airline.name' with top categories\n",
    "    df = encode_airline_names(df, top_n=10)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_all_categorical_variables(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes all relevant categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    df = encode_categorical_variables(df)\n",
    "    df = encode_codeshare_airline_names(df, top_n=top_n)\n",
    "    return df\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=50):\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame where the percentage of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    cols_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped columns with >{threshold}% missing values: {cols_to_drop}\")\n",
    "    return df\n",
    "\n",
    "def impute_datetime_columns(df, datetime_cols):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime and imputes missing values based on a logical hierarchy.\n",
    "    \"\"\"\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "            print(f\"Converted '{col}' to datetime.\")\n",
    "\n",
    "    # Impute 'departure.estimatedTime' and 'arrival.estimatedTime' with median\n",
    "    for col in ['departure.estimatedTime', 'arrival.estimatedTime']:\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            median_time = df[col].median()\n",
    "            df[col].fillna(median_time, inplace=True)\n",
    "            print(f\"Imputed missing '{col}' with median time {median_time}.\")\n",
    "\n",
    "    # Now, impute 'departure.actualTime' with 'departure.estimatedTime'\n",
    "    if 'departure.actualTime' in df.columns and 'departure.estimatedTime' in df.columns:\n",
    "        df['departure.actualTime'].fillna(df['departure.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\")\n",
    "\n",
    "    # Similarly, impute 'arrival.actualTime' with 'arrival.estimatedTime'\n",
    "    if 'arrival.actualTime' in df.columns and 'arrival.estimatedTime' in df.columns:\n",
    "        df['arrival.actualTime'].fillna(df['arrival.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'arrival.actualTime' with 'arrival.estimatedTime'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_remaining_columns(df):\n",
    "    \"\"\"\n",
    "    Imputes remaining numerical columns with median and categorical columns with 'Unknown'.\n",
    "    \"\"\"\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_value = df[col].median()\n",
    "            df[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputed missing values in numerical column '{col}' with median value {median_value}.\")\n",
    "\n",
    "    # Impute categorical columns with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"Imputed missing values in categorical column '{col}' with 'Unknown'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_low_variance_features(df, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Removes numerical features with variance below the specified threshold.\n",
    "    Non-numerical columns are retained without modification.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - threshold (float): The variance threshold.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with low variance numerical features removed.\n",
    "    \"\"\"\n",
    "    # Select numerical columns (int64, float64, uint8)\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "\n",
    "    if not numeric_cols:\n",
    "        print(\"No numerical columns to apply VarianceThreshold.\")\n",
    "        return df\n",
    "\n",
    "    # Initialize VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "    # Fit the selector on numerical data\n",
    "    try:\n",
    "        selector.fit(df[numeric_cols])\n",
    "    except ValueError as e:\n",
    "        print(f\"VarianceThreshold failed: {e}\")\n",
    "        return df\n",
    "\n",
    "    # Get the columns to keep\n",
    "    features_to_keep = [col for col, keep in zip(numeric_cols, selector.get_support()) if keep]\n",
    "\n",
    "    # Retain selected numerical features and all non-numerical features\n",
    "    non_numeric_cols = df.columns.difference(numeric_cols)\n",
    "    df_filtered = pd.concat([df[non_numeric_cols], df[features_to_keep]], axis=1)\n",
    "\n",
    "    print(f\"Removed low variance features. Remaining features: {len(df_filtered.columns)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def load_and_preprocess_weather_data(weather_directory, num_files=13):\n",
    "    \"\"\"\n",
    "    Loads, reshapes, and concatenates weather data from multiple Excel files.\n",
    "    \n",
    "    Parameters:\n",
    "    - weather_directory (str): Path to the directory containing weather Excel files.\n",
    "    - num_files (int): Number of Excel files to load (e.g., first 13 files).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined and reshaped weather data.\n",
    "    \"\"\"\n",
    "    weather_dfs = []\n",
    "    start_month = 7  # Assuming file 1.xlsx is July\n",
    "    start_year = 2023  # Starting year\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_name = f\"{i}.xlsx\"\n",
    "        file_path = os.path.join(weather_directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df_weather = pd.read_excel(file_path, header=None)\n",
    "                print(f\"Loaded weather data from {file_name}\")\n",
    "                \n",
    "                # Determine month and year\n",
    "                month = (start_month + i - 1) % 12 + 1\n",
    "                year = start_year + (start_month + i - 1) // 12\n",
    "                \n",
    "                # Reshape weather data with correct year and month\n",
    "                reshaped_weather = reshape_weather_data(df_weather, year=year, month=month)\n",
    "                \n",
    "                if not reshaped_weather.empty:\n",
    "                    weather_dfs.append(reshaped_weather)\n",
    "                else:\n",
    "                    print(f\"Reshaped weather data from {file_name} is empty. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Weather file {file_name} does not exist in {weather_directory}. Skipping.\")\n",
    "    \n",
    "    if weather_dfs:\n",
    "        combined_weather_df = pd.concat(weather_dfs, ignore_index=True)\n",
    "        print(f\"Combined weather DataFrame shape: {combined_weather_df.shape}\")\n",
    "        return combined_weather_df\n",
    "    else:\n",
    "        print(\"No weather data files loaded or reshaped successfully.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def reshape_weather_data(weather_df, year=2023, month=7):\n",
    "    \"\"\"\n",
    "    Reshapes the weather data from wide to long format, handling multi-row metrics.\n",
    "\n",
    "    Assumes that:\n",
    "    - The first row contains 'Time' and day identifiers (e.g., 'Jul 1', '2', '3', ..., '14')\n",
    "    - Subsequent rows contain metrics and their submetrics (e.g., 'Temperature (°F)', 'Max', 'Avg', 'Min')\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data without headers.\n",
    "    - year (int): Year for the dates.\n",
    "    - month (int): Month number for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Reshaped weather data with one row per day and separate columns for each metric.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    data = []\n",
    "    current_metric = None\n",
    "    submetrics = ['Max', 'Avg', 'Min', 'Total']  # Include 'Total' for Precipitation\n",
    "\n",
    "    # Extract day identifiers from the first row\n",
    "    days = weather_df.iloc[0, 1:].tolist()  # Skip first column 'Time'\n",
    "\n",
    "    # Convert day numbers to proper day strings\n",
    "    month_abbr = pd.to_datetime(f'{month}', format='%m').strftime('%b')\n",
    "    processed_days = []\n",
    "    for day in days:\n",
    "        if isinstance(day, str) and re.match(r'^[A-Za-z]+', day):\n",
    "            processed_days.append(day)\n",
    "        else:\n",
    "            processed_days.append(f\"{month_abbr} {day}\")\n",
    "\n",
    "    # Iterate through the rest of the rows\n",
    "    for index, row in weather_df.iloc[1:].iterrows():\n",
    "        metric = row[0]\n",
    "        if pd.isnull(metric):\n",
    "            continue  # Skip empty rows\n",
    "\n",
    "        if metric not in submetrics:\n",
    "            # This row defines a new main metric\n",
    "            current_metric = metric\n",
    "            continue\n",
    "\n",
    "        # This row defines a submetric under the current main metric\n",
    "        submetric = metric\n",
    "        if current_metric is None:\n",
    "            print(f\"Submetric '{submetric}' found without a main metric at row {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through day columns\n",
    "        for day_idx, value in enumerate(row[1:]):\n",
    "            if day_idx >= len(processed_days):\n",
    "                continue  # Skip if day index exceeds available days\n",
    "\n",
    "            date_str = processed_days[day_idx]\n",
    "            try:\n",
    "                date = pd.to_datetime(f\"{date_str} {year}\", format='%b %d %Y', errors='coerce')\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing date '{date_str} {year}': {e}\")\n",
    "                date = pd.NaT\n",
    "\n",
    "            if pd.isnull(date):\n",
    "                continue  # Skip invalid dates\n",
    "\n",
    "            # Define full metric name\n",
    "            if submetric == 'Total':\n",
    "                full_metric = 'Precipitation (in)'\n",
    "            else:\n",
    "                full_metric = f\"{current_metric} ({submetric})\"\n",
    "\n",
    "            # Append to data\n",
    "            data.append({'Date': date, 'Metric': full_metric, 'Value': value})\n",
    "\n",
    "    # Create long-format DataFrame\n",
    "    weather_long_df = pd.DataFrame(data)\n",
    "\n",
    "    # Pivot to wide format\n",
    "    if weather_long_df.empty:\n",
    "        print(\"No valid weather data to reshape.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    weather_pivot = weather_long_df.pivot_table(\n",
    "        index='Date',\n",
    "        columns='Metric',\n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Convert metric columns to numeric types\n",
    "    for col in weather_pivot.columns:\n",
    "        if col != 'Date':\n",
    "            weather_pivot[col] = pd.to_numeric(weather_pivot[col], errors='coerce')\n",
    "\n",
    "    print(f\"Reshaped weather DataFrame shape: {weather_pivot.shape}\")\n",
    "    return weather_pivot\n",
    "\n",
    "def preprocess_weather_data(weather_df, year=2023, month=7):\n",
    "    \"\"\"\n",
    "    Preprocesses the weather data DataFrame.\n",
    "\n",
    "    - Reshapes the data from wide to long format.\n",
    "    - Parses 'Date' column to datetime.\n",
    "    - Handles missing values if any.\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data without headers.\n",
    "    - year (int): Year for the dates.\n",
    "    - month (int): Month number for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed weather data.\n",
    "    \"\"\"\n",
    "    # Reshape the weather data\n",
    "    weather_pivot = reshape_weather_data(weather_df, year=year, month=month)\n",
    "\n",
    "    # Verify the reshaped data\n",
    "    if weather_pivot.empty:\n",
    "        print(\"Reshaping resulted in an empty DataFrame.\")\n",
    "        return weather_pivot\n",
    "\n",
    "    return weather_pivot\n",
    "\n",
    "def merge_flight_weather(flight_df, weather_df):\n",
    "    \"\"\"\n",
    "    Merges flight data with weather data based on departure date.\n",
    "    \n",
    "    Parameters:\n",
    "    - flight_df (pd.DataFrame): Cleaned flight data with 'departure.scheduledTime'.\n",
    "    - weather_df (pd.DataFrame): Cleaned and reshaped weather data with 'Date'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'departure.scheduledTime' is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(flight_df['departure.scheduledTime']):\n",
    "        flight_df['departure.scheduledTime'] = pd.to_datetime(flight_df['departure.scheduledTime'], errors='coerce')\n",
    "        print(\"Converted 'departure.scheduledTime' to datetime.\")\n",
    "    \n",
    "    # Create a 'Departure Date' column (date only, no time)\n",
    "    flight_df['Departure Date'] = flight_df['departure.scheduledTime'].dt.date\n",
    "    weather_df['Date'] = weather_df['Date'].dt.date  # Ensure 'Date' is date only\n",
    "\n",
    "    # Merge on 'Departure Date' and 'Date'\n",
    "    merged_df = pd.merge(\n",
    "        flight_df,\n",
    "        weather_df,\n",
    "        how='left',\n",
    "        left_on='Departure Date',\n",
    "        right_on='Date',\n",
    "        suffixes=('', '_weather')\n",
    "    )\n",
    "\n",
    "    # Drop the redundant 'Date' column from weather data\n",
    "    merged_df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    print(f\"Merged flight data with weather data. Merged DataFrame shape: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "def encode_and_save_label_encoder(df, column, encoder_filename):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column using LabelEncoder and saves the encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the column.\n",
    "    - column (str): The name of the column to encode.\n",
    "    - encoder_filename (str): The filename to save the encoder.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the encoded column.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{column}_encoded'] = le.fit_transform(df[column])\n",
    "        joblib.dump(le, encoder_filename)\n",
    "        print(f\"Encoded '{column}' column and saved LabelEncoder as '{encoder_filename}'.\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Specify the directories containing your data\n",
    "    flight_data_directory = 'ML-Proj-Dataset/Train/'    # Replace with your actual flight data path\n",
    "    weather_data_directory = 'ML-Proj-Dataset/Weather/'         # Replace with your actual weather data path\n",
    "\n",
    "    # Check if the directories exist\n",
    "    if not os.path.isdir(flight_data_directory):\n",
    "        print(f\"The directory '{flight_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "    if not os.path.isdir(weather_data_directory):\n",
    "        print(f\"The directory '{weather_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Process the .docx flight data files and get the combined DataFrame\n",
    "    flight_df = process_docx_files(flight_data_directory)\n",
    "\n",
    "    if flight_df.empty:\n",
    "        print(\"No flight data to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nInitial Flight DataFrame shape: {flight_df.shape}\")\n",
    "\n",
    "    # Define datetime columns in flight data\n",
    "    flight_datetime_cols = [\n",
    "        'departure.scheduledTime',\n",
    "        'departure.estimatedTime',\n",
    "        'departure.actualTime',\n",
    "        'departure.estimatedRunway',\n",
    "        'departure.actualRunway',\n",
    "        'arrival.scheduledTime',\n",
    "        'arrival.estimatedTime',\n",
    "        'arrival.actualTime',\n",
    "        'arrival.estimatedRunway',\n",
    "        'arrival.actualRunway'\n",
    "    ]\n",
    "\n",
    "    # Drop columns with >50% missing values\n",
    "    flight_df = drop_high_missing_columns(flight_df, threshold=50)\n",
    "\n",
    "    # Convert and impute datetime columns\n",
    "    flight_df = impute_datetime_columns(flight_df, flight_datetime_cols)\n",
    "\n",
    "    # Impute remaining numerical and categorical columns\n",
    "    flight_df = impute_remaining_columns(flight_df)\n",
    "\n",
    "    # Encode categorical variables with optimized methods\n",
    "    flight_df = encode_all_categorical_variables(flight_df, top_n=10)\n",
    "\n",
    "    # Remove low variance numerical features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.01)  # Adjust threshold as needed\n",
    "\n",
    "    # Calculate departure delay if possible\n",
    "    if 'departure.actualTime' in flight_df.columns and 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.delay_minutes'] = (flight_df['departure.actualTime'] - flight_df['departure.scheduledTime']).dt.total_seconds() / 60\n",
    "        print(\"Calculated 'departure.delay_minutes'.\")\n",
    "\n",
    "    # Extract temporal features if needed\n",
    "    if 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.day_of_week'] = flight_df['departure.scheduledTime'].dt.day_name()\n",
    "        flight_df['departure.hour_of_day'] = flight_df['departure.scheduledTime'].dt.hour\n",
    "        flight_df['departure.month'] = flight_df['departure.scheduledTime'].dt.month\n",
    "        print(\"Extracted temporal features.\")\n",
    "\n",
    "        # One-Hot Encode temporal features\n",
    "        flight_df = pd.get_dummies(flight_df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # Remove low variance numerical features again after adding temporal features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.0)  # Remove features with zero variance\n",
    "\n",
    "    # Impute 'departure.delay_minutes' with median if there are still missing values\n",
    "    if 'departure.delay_minutes' in flight_df.columns and flight_df['departure.delay_minutes'].isnull().sum() > 0:\n",
    "        median_delay = flight_df['departure.delay_minutes'].median()\n",
    "        flight_df['departure.delay_minutes'].fillna(median_delay, inplace=True)\n",
    "        print(f\"Imputed missing 'departure.delay_minutes' with median value {median_delay}.\")\n",
    "\n",
    "    # Load and preprocess weather data\n",
    "    weather_df = load_and_preprocess_weather_data(weather_data_directory, num_files=13)\n",
    "    if weather_df.empty:\n",
    "        print(\"No weather data available for merging.\")\n",
    "    else:\n",
    "        # Merge flight data with weather data\n",
    "        flight_df = merge_flight_weather(flight_df, weather_df)\n",
    "\n",
    "        # Handle any remaining missing weather data by imputing with median\n",
    "        weather_metrics = ['Temperature (°F)', 'Dew Point (°F)', 'Humidity (%)', \n",
    "                           'Wind Speed (mph)', 'Pressure (in)', 'Precipitation (in)']\n",
    "        \n",
    "        for metric in weather_metrics:\n",
    "            if metric in flight_df.columns:\n",
    "                if flight_df[metric].isnull().sum() > 0:\n",
    "                    median_value = flight_df[metric].median()\n",
    "                    flight_df[metric].fillna(median_value, inplace=True)\n",
    "                    print(f\"Imputed missing '{metric}' with median value {median_value}.\")\n",
    "\n",
    "    # Save LabelEncoder for 'status'\n",
    "    if 'status_encoded' in flight_df.columns:\n",
    "        le_status = LabelEncoder()\n",
    "        le_status.fit(flight_df['status'])\n",
    "        joblib.dump(le_status, 'label_encoder_status.pkl')\n",
    "        print(\"Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\")\n",
    "\n",
    "    # Save the final cleaned and merged DataFrame\n",
    "    flight_df.to_csv('final_cleaned_flight_data_with_weather.csv', index=False)\n",
    "    print(\"\\nFinal cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\")\n",
    "\n",
    "    # Display final DataFrame information\n",
    "    print(\"\\nFinal DataFrame shape:\", flight_df.shape)\n",
    "    print(\"\\nFinal DataFrame Info:\")\n",
    "    print(flight_df.info())\n",
    "\n",
    "    print(\"\\nMissing Values After All Processing:\")\n",
    "    print(flight_df.isnull().sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be419a81-c0b5-4415-be0f-a64793bdbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1.docx\n",
      "Processing file: 10.docx\n",
      "Processing file: 11.docx\n",
      "Processing file: 12.docx\n",
      "Processing file: 13.docx\n",
      "Processing file: 14.docx\n",
      "Processing file: 15.docx\n",
      "Processing file: 16.docx\n",
      "Processing file: 17.docx\n",
      "Processing file: 18.docx\n",
      "Processing file: 19.docx\n",
      "Processing file: 2.docx\n",
      "Processing file: 20.docx\n",
      "Processing file: 21.docx\n",
      "Processing file: 22.docx\n",
      "Processing file: 23.docx\n",
      "Processing file: 24.docx\n",
      "Processing file: 25.docx\n",
      "Processing file: 26.docx\n",
      "Processing file: 27.docx\n",
      "Processing file: 28.docx\n",
      "Processing file: 29.docx\n",
      "Processing file: 3.docx\n",
      "Processing file: 30.docx\n",
      "Processing file: 31.docx\n",
      "Processing file: 32.docx\n",
      "Processing file: 33.docx\n",
      "Processing file: 34.docx\n",
      "Processing file: 35.docx\n",
      "Processing file: 36.docx\n",
      "Processing file: 37.docx\n",
      "Processing file: 39.docx\n",
      "Processing file: 4.docx\n",
      "Processing file: 40.docx\n",
      "Processing file: 41.docx\n",
      "Processing file: 42.docx\n",
      "Processing file: 43.docx\n",
      "Processing file: 44.docx\n",
      "Processing file: 45.docx\n",
      "Processing file: 46.docx\n",
      "Processing file: 47.docx\n",
      "Processing file: 48.docx\n",
      "Processing file: 49.docx\n",
      "Processing file: 5.docx\n",
      "Processing file: 50.docx\n",
      "Processing file: 51.docx\n",
      "Processing file: 52.docx\n",
      "Processing file: 53.docx\n",
      "Processing file: 54.docx\n",
      "Processing file: 55.docx\n",
      "Processing file: 56.docx\n",
      "Processing file: 57.docx\n",
      "Processing file: 58.docx\n",
      "Processing file: 59.docx\n",
      "Processing file: 6.docx\n",
      "Processing file: 60.docx\n",
      "Processing file: 61.docx\n",
      "Processing file: 62.docx\n",
      "Processing file: 63.docx\n",
      "Processing file: 64.docx\n",
      "Processing file: 65.docx\n",
      "Processing file: 66.docx\n",
      "Processing file: 67.docx\n",
      "Processing file: 68.docx\n",
      "Processing file: 69.docx\n",
      "Processing file: 7.docx\n",
      "Processing file: 70.docx\n",
      "Processing file: 71.docx\n",
      "Processing file: 72.docx\n",
      "Processing file: 8.docx\n",
      "Processing file: 9.docx\n",
      "\n",
      "Initial Flight DataFrame shape: (51572, 33)\n",
      "Dropped columns with >50% missing values: ['arrival.terminal', 'arrival.baggage', 'codeshared.airline.name', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.flight.number', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'arrival.gate', 'departure.gate', 'arrival.actualTime', 'arrival.estimatedRunway', 'arrival.actualRunway']\n",
      "Converted 'departure.scheduledTime' to datetime.\n",
      "Converted 'departure.estimatedTime' to datetime.\n",
      "Converted 'departure.actualTime' to datetime.\n",
      "Converted 'departure.estimatedRunway' to datetime.\n",
      "Converted 'departure.actualRunway' to datetime.\n",
      "Converted 'arrival.scheduledTime' to datetime.\n",
      "Converted 'arrival.estimatedTime' to datetime.\n",
      "Imputed missing 'departure.estimatedTime' with median time 2024-01-04 06:00:00.\n",
      "Imputed missing 'arrival.estimatedTime' with median time 2024-01-13 15:19:00.\n",
      "Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\n",
      "Imputed missing values in categorical column 'departure.terminal' with 'Unknown'.\n",
      "Encoded 'status' column.\n",
      "One-Hot Encoded 'airline.name' with top 10 categories.\n",
      "No 'codeshared.airline.name' column found to encode.\n",
      "No numerical columns to apply VarianceThreshold.\n",
      "Calculated 'departure.delay_minutes'.\n",
      "Extracted temporal features.\n",
      "One-Hot Encoded 'departure.day_of_week'.\n",
      "Removed low variance features. Remaining features: 39\n",
      "Loaded weather data from 1.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 1.xlsx is empty. Skipping.\n",
      "Loaded weather data from 2.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 2.xlsx is empty. Skipping.\n",
      "Loaded weather data from 3.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 3.xlsx is empty. Skipping.\n",
      "Loaded weather data from 4.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 4.xlsx is empty. Skipping.\n",
      "Loaded weather data from 5.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 5.xlsx is empty. Skipping.\n",
      "Loaded weather data from 6.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 6.xlsx is empty. Skipping.\n",
      "Loaded weather data from 7.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 7.xlsx is empty. Skipping.\n",
      "Loaded weather data from 8.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 8.xlsx is empty. Skipping.\n",
      "Loaded weather data from 9.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 9.xlsx is empty. Skipping.\n",
      "Loaded weather data from 10.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 10.xlsx is empty. Skipping.\n",
      "Loaded weather data from 11.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 11.xlsx is empty. Skipping.\n",
      "Loaded weather data from 12.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 12.xlsx is empty. Skipping.\n",
      "Loaded weather data from 13.xlsx\n",
      "No valid weather data to reshape.\n",
      "Reshaped weather data from 13.xlsx is empty. Skipping.\n",
      "No weather data files loaded or reshaped successfully.\n",
      "No weather data available for merging.\n",
      "Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\n",
      "\n",
      "Final cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\n",
      "\n",
      "Final DataFrame shape: (51572, 39)\n",
      "\n",
      "Final DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51572 entries, 0 to 51571\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                   Non-Null Count  Dtype         \n",
      "---  ------                                   --------------  -----         \n",
      " 0   airline.iataCode                         51572 non-null  object        \n",
      " 1   airline.icaoCode                         51572 non-null  object        \n",
      " 2   airline_airblue                          51572 non-null  bool          \n",
      " 3   airline_airsial                          51572 non-null  bool          \n",
      " 4   airline_british airways                  51572 non-null  bool          \n",
      " 5   airline_emirates                         51572 non-null  bool          \n",
      " 6   airline_flyjinnah                        51572 non-null  bool          \n",
      " 7   airline_klm                              51572 non-null  bool          \n",
      " 8   airline_oman air                         51572 non-null  bool          \n",
      " 9   airline_pakistan international airlines  51572 non-null  bool          \n",
      " 10  airline_qatar airways                    51572 non-null  bool          \n",
      " 11  airline_serene air                       51572 non-null  bool          \n",
      " 12  arrival.estimatedTime                    51572 non-null  datetime64[ns]\n",
      " 13  arrival.iataCode                         51572 non-null  object        \n",
      " 14  arrival.icaoCode                         51572 non-null  object        \n",
      " 15  arrival.scheduledTime                    51572 non-null  datetime64[ns]\n",
      " 16  departure.actualRunway                   30990 non-null  datetime64[ns]\n",
      " 17  departure.actualTime                     51572 non-null  datetime64[ns]\n",
      " 18  departure.day_of_week_Monday             51572 non-null  bool          \n",
      " 19  departure.day_of_week_Saturday           51572 non-null  bool          \n",
      " 20  departure.day_of_week_Sunday             51572 non-null  bool          \n",
      " 21  departure.day_of_week_Thursday           51572 non-null  bool          \n",
      " 22  departure.day_of_week_Tuesday            51572 non-null  bool          \n",
      " 23  departure.day_of_week_Wednesday          51572 non-null  bool          \n",
      " 24  departure.estimatedRunway                30990 non-null  datetime64[ns]\n",
      " 25  departure.estimatedTime                  51572 non-null  datetime64[ns]\n",
      " 26  departure.hour_of_day                    51572 non-null  int32         \n",
      " 27  departure.iataCode                       51572 non-null  object        \n",
      " 28  departure.icaoCode                       51572 non-null  object        \n",
      " 29  departure.month                          51572 non-null  int32         \n",
      " 30  departure.scheduledTime                  51572 non-null  datetime64[ns]\n",
      " 31  departure.terminal                       51572 non-null  object        \n",
      " 32  flight.iataNumber                        51572 non-null  object        \n",
      " 33  flight.icaoNumber                        51572 non-null  object        \n",
      " 34  flight.number                            51572 non-null  object        \n",
      " 35  status                                   51572 non-null  object        \n",
      " 36  status_encoded                           51572 non-null  int32         \n",
      " 37  type                                     51572 non-null  object        \n",
      " 38  departure.delay_minutes                  51572 non-null  float64       \n",
      "dtypes: bool(16), datetime64[ns](7), float64(1), int32(3), object(12)\n",
      "memory usage: 9.2+ MB\n",
      "None\n",
      "\n",
      "Missing Values After All Processing:\n",
      "airline.iataCode                               0\n",
      "airline.icaoCode                               0\n",
      "airline_airblue                                0\n",
      "airline_airsial                                0\n",
      "airline_british airways                        0\n",
      "airline_emirates                               0\n",
      "airline_flyjinnah                              0\n",
      "airline_klm                                    0\n",
      "airline_oman air                               0\n",
      "airline_pakistan international airlines        0\n",
      "airline_qatar airways                          0\n",
      "airline_serene air                             0\n",
      "arrival.estimatedTime                          0\n",
      "arrival.iataCode                               0\n",
      "arrival.icaoCode                               0\n",
      "arrival.scheduledTime                          0\n",
      "departure.actualRunway                     20582\n",
      "departure.actualTime                           0\n",
      "departure.day_of_week_Monday                   0\n",
      "departure.day_of_week_Saturday                 0\n",
      "departure.day_of_week_Sunday                   0\n",
      "departure.day_of_week_Thursday                 0\n",
      "departure.day_of_week_Tuesday                  0\n",
      "departure.day_of_week_Wednesday                0\n",
      "departure.estimatedRunway                  20582\n",
      "departure.estimatedTime                        0\n",
      "departure.hour_of_day                          0\n",
      "departure.iataCode                             0\n",
      "departure.icaoCode                             0\n",
      "departure.month                                0\n",
      "departure.scheduledTime                        0\n",
      "departure.terminal                             0\n",
      "flight.iataNumber                              0\n",
      "flight.icaoNumber                              0\n",
      "flight.number                                  0\n",
      "status                                         0\n",
      "status_encoded                                 0\n",
      "type                                           0\n",
      "departure.delay_minutes                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "\n",
    "def extract_json_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Extracts JSON string from a .docx file.\n",
    "    Assumes that the JSON content is enclosed within [ ] brackets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    full_text = []\n",
    "    json_started = False\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        # Detect the start of JSON array\n",
    "        if text.startswith('['):\n",
    "            json_started = True\n",
    "\n",
    "        if json_started:\n",
    "            full_text.append(text)\n",
    "            # Detect the end of JSON array\n",
    "            if text.endswith(']'):\n",
    "                break\n",
    "\n",
    "    json_str = '\\n'.join(full_text)\n",
    "\n",
    "    # Optional: Clean up the JSON string using regex if necessary\n",
    "    # For example, remove unwanted characters or fix formatting issues\n",
    "    json_str = re.sub(r'(?<!\\\\)\"', r'\"', json_str)  # Replace unescaped quotes if necessary\n",
    "\n",
    "    return json_str\n",
    "\n",
    "def parse_json_to_dataframe(json_str):\n",
    "    \"\"\"\n",
    "    Parses a JSON string to a pandas DataFrame.\n",
    "    Handles both single JSON array and multiple JSON objects.\n",
    "    \"\"\"\n",
    "    if not json_str:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Attempt to load the JSON string as a list\n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, list):\n",
    "            df = pd.json_normalize(data, sep='.')\n",
    "            return df\n",
    "        else:\n",
    "            # If not a list, wrap it into a list\n",
    "            df = pd.json_normalize([data], sep='.')\n",
    "            return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_docx_files(directory_path):\n",
    "    \"\"\"\n",
    "    Processes all .docx files in the specified directory.\n",
    "    Returns a combined pandas DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            json_str = extract_json_from_docx(file_path)\n",
    "            df = parse_json_to_dataframe(json_str)\n",
    "            if df is not None:\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                print(f\"Failed to parse JSON in file: {filename}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No DataFrames to concatenate.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def encode_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    if 'airline.name' in df.columns:\n",
    "        top_airlines = df['airline.name'].value_counts().nlargest(top_n).index\n",
    "        df['airline.name'] = df['airline.name'].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "        df = pd.get_dummies(df, columns=['airline.name'], prefix='airline', drop_first=True)\n",
    "        print(f\"One-Hot Encoded 'airline.name' with top {top_n} categories.\")\n",
    "    return df\n",
    "\n",
    "def encode_codeshare_airline_names(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes 'codeshared.airline.name' by one-hot encoding the top N frequent airlines.\n",
    "    Groups the rest as 'Other'.\n",
    "    \"\"\"\n",
    "    # Adjust the column name based on actual DataFrame columns\n",
    "    possible_column_names = ['codeshared.airline.name', 'codeshare.airline.name', 'codeshare_airline.name']\n",
    "    column_found = False\n",
    "    for col in possible_column_names:\n",
    "        if col in df.columns:\n",
    "            top_airlines = df[col].value_counts().nlargest(top_n).index\n",
    "            df[col] = df[col].apply(lambda x: x if x in top_airlines else 'Other')\n",
    "            df = pd.get_dummies(df, columns=[col], prefix='codeshare_airline', drop_first=True)\n",
    "            print(f\"One-Hot Encoded '{col}' with top {top_n} categories.\")\n",
    "            column_found = True\n",
    "            break\n",
    "    if not column_found:\n",
    "        print(\"No 'codeshared.airline.name' column found to encode.\")\n",
    "    return df\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Encode 'status' column (binary or multi-class classification)\n",
    "    if 'status' in df.columns:\n",
    "        df['status_encoded'] = le.fit_transform(df['status'])\n",
    "        print(\"Encoded 'status' column.\")\n",
    "\n",
    "    # One-Hot Encode 'departure.day_of_week' if exists\n",
    "    if 'departure.day_of_week' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # One-Hot Encode 'airline.name' with top categories\n",
    "    df = encode_airline_names(df, top_n=10)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_all_categorical_variables(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Encodes all relevant categorical variables into numerical formats.\n",
    "    \"\"\"\n",
    "    df = encode_categorical_variables(df)\n",
    "    df = encode_codeshare_airline_names(df, top_n=top_n)\n",
    "    return df\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=50):\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame where the percentage of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    missing_percentage = df.isnull().mean() * 100\n",
    "    cols_to_drop = missing_percentage[missing_percentage > threshold].index.tolist()\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Dropped columns with >{threshold}% missing values: {cols_to_drop}\")\n",
    "    return df\n",
    "\n",
    "def impute_datetime_columns(df, datetime_cols):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime and imputes missing values based on a logical hierarchy.\n",
    "    \"\"\"\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "            print(f\"Converted '{col}' to datetime.\")\n",
    "\n",
    "    # Impute 'departure.estimatedTime' and 'arrival.estimatedTime' with median\n",
    "    for col in ['departure.estimatedTime', 'arrival.estimatedTime']:\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            median_time = df[col].median()\n",
    "            df[col].fillna(median_time, inplace=True)\n",
    "            print(f\"Imputed missing '{col}' with median time {median_time}.\")\n",
    "\n",
    "    # Now, impute 'departure.actualTime' with 'departure.estimatedTime'\n",
    "    if 'departure.actualTime' in df.columns and 'departure.estimatedTime' in df.columns:\n",
    "        df['departure.actualTime'].fillna(df['departure.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'departure.actualTime' with 'departure.estimatedTime'.\")\n",
    "\n",
    "    # Similarly, impute 'arrival.actualTime' with 'arrival.estimatedTime'\n",
    "    if 'arrival.actualTime' in df.columns and 'arrival.estimatedTime' in df.columns:\n",
    "        df['arrival.actualTime'].fillna(df['arrival.estimatedTime'], inplace=True)\n",
    "        print(\"Imputed missing 'arrival.actualTime' with 'arrival.estimatedTime'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_remaining_columns(df):\n",
    "    \"\"\"\n",
    "    Imputes remaining numerical columns with median and categorical columns with 'Unknown'.\n",
    "    \"\"\"\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            median_value = df[col].median()\n",
    "            df[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputed missing values in numerical column '{col}' with median value {median_value}.\")\n",
    "\n",
    "    # Impute categorical columns with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"Imputed missing values in categorical column '{col}' with 'Unknown'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_low_variance_features(df, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Removes numerical features with variance below the specified threshold.\n",
    "    Non-numerical columns are retained without modification.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - threshold (float): The variance threshold.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with low variance numerical features removed.\n",
    "    \"\"\"\n",
    "    # Select numerical columns (int64, float64, uint8)\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64', 'uint8']).columns.tolist()\n",
    "\n",
    "    if not numeric_cols:\n",
    "        print(\"No numerical columns to apply VarianceThreshold.\")\n",
    "        return df\n",
    "\n",
    "    # Initialize VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "    # Fit the selector on numerical data\n",
    "    try:\n",
    "        selector.fit(df[numeric_cols])\n",
    "    except ValueError as e:\n",
    "        print(f\"VarianceThreshold failed: {e}\")\n",
    "        return df\n",
    "\n",
    "    # Get the columns to keep\n",
    "    features_to_keep = [col for col, keep in zip(numeric_cols, selector.get_support()) if keep]\n",
    "\n",
    "    # Retain selected numerical features and all non-numerical features\n",
    "    non_numeric_cols = df.columns.difference(numeric_cols)\n",
    "    df_filtered = pd.concat([df[non_numeric_cols], df[features_to_keep]], axis=1)\n",
    "\n",
    "    print(f\"Removed low variance features. Remaining features: {len(df_filtered.columns)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def load_and_preprocess_weather_data(weather_directory, num_files=13):\n",
    "    \"\"\"\n",
    "    Loads, reshapes, and concatenates weather data from multiple Excel files.\n",
    "    \n",
    "    Parameters:\n",
    "    - weather_directory (str): Path to the directory containing weather Excel files.\n",
    "    - num_files (int): Number of Excel files to load (e.g., first 13 files).\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined and reshaped weather data.\n",
    "    \"\"\"\n",
    "    weather_dfs = []\n",
    "    start_month = 7  # Assuming file 1.xlsx is July\n",
    "    start_year = 2023  # Starting year\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_name = f\"{i}.xlsx\"\n",
    "        file_path = os.path.join(weather_directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Read the Excel file without headers\n",
    "                df_weather = pd.read_excel(file_path, header=None)\n",
    "                print(f\"Loaded weather data from {file_name}\")\n",
    "                \n",
    "                # Determine month and year\n",
    "                month = (start_month + i - 1) % 12 + 1\n",
    "                year = start_year + (start_month + i - 1) // 12\n",
    "                \n",
    "                # Reshape weather data with correct year and month\n",
    "                reshaped_weather = reshape_weather_data(df_weather, year=year, month=month)\n",
    "                \n",
    "                if not reshaped_weather.empty:\n",
    "                    weather_dfs.append(reshaped_weather)\n",
    "                else:\n",
    "                    print(f\"Reshaped weather data from {file_name} is empty. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Weather file {file_name} does not exist in {weather_directory}. Skipping.\")\n",
    "    \n",
    "    if weather_dfs:\n",
    "        combined_weather_df = pd.concat(weather_dfs, ignore_index=True)\n",
    "        print(f\"Combined weather DataFrame shape: {combined_weather_df.shape}\")\n",
    "        return combined_weather_df\n",
    "    else:\n",
    "        print(\"No weather data files loaded or reshaped successfully.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def reshape_weather_data(weather_df, year=2023, month=7):\n",
    "    \"\"\"\n",
    "    Reshapes the weather data from wide to long format, handling multi-row metrics.\n",
    "\n",
    "    Assumes that:\n",
    "    - The first row contains 'Time' and day identifiers (e.g., 'Jul 1', '2', '3', ..., '14')\n",
    "    - Subsequent rows contain metrics and their submetrics (e.g., 'Temperature (°F)', 'Max', 'Avg', 'Min')\n",
    "    - Submetrics are listed in a single cell separated by spaces (e.g., 'Max Avg Min')\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data without headers.\n",
    "    - year (int): Year for the dates.\n",
    "    - month (int): Month number for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Reshaped weather data with one row per day and separate columns for each metric.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_metric = None\n",
    "    submetrics = []\n",
    "\n",
    "    # Extract day identifiers from the second row\n",
    "    days_row = weather_df.iloc[1, 1:].tolist()  # Skip first column 'Time'\n",
    "    month_abbr = pd.to_datetime(f'{month}', format='%m').strftime('%b')\n",
    "    processed_days = []\n",
    "    for day in days_row:\n",
    "        if isinstance(day, str) and re.match(r'^[A-Za-z]+', day):\n",
    "            processed_days.append(day)\n",
    "        else:\n",
    "            processed_days.append(f\"{month_abbr} {day}\")\n",
    "\n",
    "    # Iterate over the data starting from the third row\n",
    "    row_idx = 2\n",
    "    total_rows = weather_df.shape[0]\n",
    "    while row_idx < total_rows:\n",
    "        metric_name = weather_df.iloc[row_idx, 0]\n",
    "        if pd.isnull(metric_name):\n",
    "            row_idx += 1\n",
    "            continue  # Skip empty rows\n",
    "        current_metric = metric_name\n",
    "        row_idx += 1\n",
    "        if row_idx >= total_rows:\n",
    "            break  # Prevent index out of range\n",
    "\n",
    "        submetric_row = weather_df.iloc[row_idx, :].tolist()\n",
    "        submetric_names = []\n",
    "        submetric_values = []\n",
    "        \n",
    "        # Assuming the first cell contains all submetric names separated by space\n",
    "        submetrics_cell = submetric_row[0]\n",
    "        if isinstance(submetrics_cell, str):\n",
    "            submetric_names = submetrics_cell.split()\n",
    "        else:\n",
    "            print(f\"Unexpected submetric format at row {row_idx}: {submetrics_cell}\")\n",
    "            row_idx += 1\n",
    "            continue\n",
    "\n",
    "        # The rest of the cells contain values\n",
    "        values = submetric_row[1:]\n",
    "        # Ensure that the number of values matches the number of submetrics times the number of days\n",
    "        expected_values = len(submetric_names) * len(processed_days)\n",
    "        if len(values) < expected_values:\n",
    "            print(f\"Not enough values for metric '{current_metric}'. Expected {expected_values}, got {len(values)}.\")\n",
    "            row_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Iterate through each day and submetric\n",
    "        for day_idx, day in enumerate(processed_days):\n",
    "            for sub_idx, submetric in enumerate(submetric_names):\n",
    "                value_idx = day_idx * len(submetric_names) + sub_idx\n",
    "                value = values[value_idx]\n",
    "                try:\n",
    "                    date = pd.to_datetime(f\"{day} {year}\", format='%b %d %Y', errors='coerce')\n",
    "                    if pd.isnull(date):\n",
    "                        print(f\"Invalid date: {day} {year}\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing date '{day} {year}': {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Define full metric name\n",
    "                if submetric.lower() == 'total':\n",
    "                    full_metric = 'Precipitation (in)'\n",
    "                else:\n",
    "                    full_metric = f\"{current_metric} ({submetric})\"\n",
    "\n",
    "                # Append to data\n",
    "                data.append({'Date': date, 'Metric': full_metric, 'Value': value})\n",
    "\n",
    "        row_idx += 1  # Move to the next metric\n",
    "\n",
    "    # Create long-format DataFrame\n",
    "    weather_long_df = pd.DataFrame(data)\n",
    "\n",
    "    # Pivot to wide format\n",
    "    if weather_long_df.empty:\n",
    "        print(\"No valid weather data to reshape.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    weather_pivot = weather_long_df.pivot_table(\n",
    "        index='Date',\n",
    "        columns='Metric',\n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Convert metric columns to numeric types\n",
    "    for col in weather_pivot.columns:\n",
    "        if col != 'Date':\n",
    "            weather_pivot[col] = pd.to_numeric(weather_pivot[col], errors='coerce')\n",
    "\n",
    "    print(f\"Reshaped weather DataFrame shape: {weather_pivot.shape}\")\n",
    "    return weather_pivot\n",
    "\n",
    "def preprocess_weather_data(weather_df, year=2023, month=7):\n",
    "    \"\"\"\n",
    "    Preprocesses the weather data DataFrame.\n",
    "\n",
    "    - Reshapes the data from wide to long format.\n",
    "    - Parses 'Date' column to datetime.\n",
    "    - Handles missing values if any.\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data without headers.\n",
    "    - year (int): Year for the dates.\n",
    "    - month (int): Month number for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed weather data.\n",
    "    \"\"\"\n",
    "    # Reshape the weather data\n",
    "    weather_pivot = reshape_weather_data(weather_df, year=year, month=month)\n",
    "\n",
    "    # Verify the reshaped data\n",
    "    if weather_pivot.empty:\n",
    "        print(\"Reshaping resulted in an empty DataFrame.\")\n",
    "        return weather_pivot\n",
    "\n",
    "    return weather_pivot\n",
    "\n",
    "def merge_flight_weather(flight_df, weather_df):\n",
    "    \"\"\"\n",
    "    Merges flight data with weather data based on departure date.\n",
    "    \n",
    "    Parameters:\n",
    "    - flight_df (pd.DataFrame): Cleaned flight data with 'departure.scheduledTime'.\n",
    "    - weather_df (pd.DataFrame): Cleaned and reshaped weather data with 'Date'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure 'departure.scheduledTime' is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(flight_df['departure.scheduledTime']):\n",
    "        flight_df['departure.scheduledTime'] = pd.to_datetime(flight_df['departure.scheduledTime'], errors='coerce')\n",
    "        print(\"Converted 'departure.scheduledTime' to datetime.\")\n",
    "    \n",
    "    # Create a 'Departure Date' column (date only, no time)\n",
    "    flight_df['Departure Date'] = flight_df['departure.scheduledTime'].dt.date\n",
    "    weather_df['Date'] = weather_df['Date'].dt.date  # Ensure 'Date' is date only\n",
    "\n",
    "    # Merge on 'Departure Date' and 'Date'\n",
    "    merged_df = pd.merge(\n",
    "        flight_df,\n",
    "        weather_df,\n",
    "        how='left',\n",
    "        left_on='Departure Date',\n",
    "        right_on='Date',\n",
    "        suffixes=('', '_weather')\n",
    "    )\n",
    "\n",
    "    # Drop the redundant 'Date' column from weather data\n",
    "    merged_df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    print(f\"Merged flight data with weather data. Merged DataFrame shape: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "def encode_and_save_label_encoder(df, column, encoder_filename):\n",
    "    \"\"\"\n",
    "    Encodes a categorical column using LabelEncoder and saves the encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the column.\n",
    "    - column (str): The name of the column to encode.\n",
    "    - encoder_filename (str): The filename to save the encoder.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the encoded column.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{column}_encoded'] = le.fit_transform(df[column])\n",
    "        joblib.dump(le, encoder_filename)\n",
    "        print(f\"Encoded '{column}' column and saved LabelEncoder as '{encoder_filename}'.\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Specify the directories containing your data\n",
    "    flight_data_directory = 'ML-Proj-Dataset/Train/'    # Replace with your actual flight data path\n",
    "    weather_data_directory = 'ML-Proj-Dataset/Weather/'         # Replace with your actual weather data path\n",
    "\n",
    "    # Check if the directories exist\n",
    "    if not os.path.isdir(flight_data_directory):\n",
    "        print(f\"The directory '{flight_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "    if not os.path.isdir(weather_data_directory):\n",
    "        print(f\"The directory '{weather_data_directory}' does not exist. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Process the .docx flight data files and get the combined DataFrame\n",
    "    flight_df = process_docx_files(flight_data_directory)\n",
    "\n",
    "    if flight_df.empty:\n",
    "        print(\"No flight data to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nInitial Flight DataFrame shape: {flight_df.shape}\")\n",
    "\n",
    "    # Define datetime columns in flight data\n",
    "    flight_datetime_cols = [\n",
    "        'departure.scheduledTime',\n",
    "        'departure.estimatedTime',\n",
    "        'departure.actualTime',\n",
    "        'departure.estimatedRunway',\n",
    "        'departure.actualRunway',\n",
    "        'arrival.scheduledTime',\n",
    "        'arrival.estimatedTime',\n",
    "        'arrival.actualTime',\n",
    "        'arrival.estimatedRunway',\n",
    "        'arrival.actualRunway'\n",
    "    ]\n",
    "\n",
    "    # Drop columns with >50% missing values\n",
    "    flight_df = drop_high_missing_columns(flight_df, threshold=50)\n",
    "\n",
    "    # Convert and impute datetime columns\n",
    "    flight_df = impute_datetime_columns(flight_df, flight_datetime_cols)\n",
    "\n",
    "    # Impute remaining numerical and categorical columns\n",
    "    flight_df = impute_remaining_columns(flight_df)\n",
    "\n",
    "    # Encode categorical variables with optimized methods\n",
    "    flight_df = encode_all_categorical_variables(flight_df, top_n=10)\n",
    "\n",
    "    # Remove low variance numerical features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.01)  # Adjust threshold as needed\n",
    "\n",
    "    # Calculate departure delay if possible\n",
    "    if 'departure.actualTime' in flight_df.columns and 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.delay_minutes'] = (flight_df['departure.actualTime'] - flight_df['departure.scheduledTime']).dt.total_seconds() / 60\n",
    "        print(\"Calculated 'departure.delay_minutes'.\")\n",
    "\n",
    "    # Extract temporal features if needed\n",
    "    if 'departure.scheduledTime' in flight_df.columns:\n",
    "        flight_df['departure.day_of_week'] = flight_df['departure.scheduledTime'].dt.day_name()\n",
    "        flight_df['departure.hour_of_day'] = flight_df['departure.scheduledTime'].dt.hour\n",
    "        flight_df['departure.month'] = flight_df['departure.scheduledTime'].dt.month\n",
    "        print(\"Extracted temporal features.\")\n",
    "\n",
    "        # One-Hot Encode temporal features\n",
    "        flight_df = pd.get_dummies(flight_df, columns=['departure.day_of_week'], drop_first=True)\n",
    "        print(\"One-Hot Encoded 'departure.day_of_week'.\")\n",
    "\n",
    "    # Remove low variance numerical features again after adding temporal features\n",
    "    flight_df = remove_low_variance_features(flight_df, threshold=0.0)  # Remove features with zero variance\n",
    "\n",
    "    # Impute 'departure.delay_minutes' with median if there are still missing values\n",
    "    if 'departure.delay_minutes' in flight_df.columns and flight_df['departure.delay_minutes'].isnull().sum() > 0:\n",
    "        median_delay = flight_df['departure.delay_minutes'].median()\n",
    "        flight_df['departure.delay_minutes'].fillna(median_delay, inplace=True)\n",
    "        print(f\"Imputed missing 'departure.delay_minutes' with median value {median_delay}.\")\n",
    "\n",
    "    # Load and preprocess weather data\n",
    "    weather_df = load_and_preprocess_weather_data(weather_data_directory, num_files=13)\n",
    "    if weather_df.empty:\n",
    "        print(\"No weather data available for merging.\")\n",
    "    else:\n",
    "        # Merge flight data with weather data\n",
    "        flight_df = merge_flight_weather(flight_df, weather_df)\n",
    "\n",
    "        # Handle any remaining missing weather data by imputing with median\n",
    "        weather_metrics = ['Temperature (°F)', 'Dew Point (°F)', 'Humidity (%)', \n",
    "                           'Wind Speed (mph)', 'Pressure (in)', 'Precipitation (in)']\n",
    "        \n",
    "        for metric in weather_metrics:\n",
    "            if metric in flight_df.columns:\n",
    "                if flight_df[metric].isnull().sum() > 0:\n",
    "                    median_value = flight_df[metric].median()\n",
    "                    flight_df[metric].fillna(median_value, inplace=True)\n",
    "                    print(f\"Imputed missing '{metric}' with median value {median_value}.\")\n",
    "\n",
    "    # Save LabelEncoder for 'status'\n",
    "    if 'status_encoded' in flight_df.columns:\n",
    "        le_status = LabelEncoder()\n",
    "        le_status.fit(flight_df['status'])\n",
    "        joblib.dump(le_status, 'label_encoder_status.pkl')\n",
    "        print(\"Saved LabelEncoder for 'status' column as 'label_encoder_status.pkl'.\")\n",
    "\n",
    "    # Save the final cleaned and merged DataFrame\n",
    "    flight_df.to_csv('final_cleaned_flight_data_with_weather.csv', index=False)\n",
    "    print(\"\\nFinal cleaned and merged DataFrame saved as 'final_cleaned_flight_data_with_weather.csv'.\")\n",
    "\n",
    "    # Display final DataFrame information\n",
    "    print(\"\\nFinal DataFrame shape:\", flight_df.shape)\n",
    "    print(\"\\nFinal DataFrame Info:\")\n",
    "    print(flight_df.info())\n",
    "\n",
    "    print(\"\\nMissing Values After All Processing:\")\n",
    "    print(flight_df.isnull().sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d192d50-b5e2-4239-9ad4-fd1d40554f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weather data from '1.xlsx'.\n",
      "Missing submetrics for metric 'Jul 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '1.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '2.xlsx'.\n",
      "Missing submetrics for metric 'Aug 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '2.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '3.xlsx'.\n",
      "Missing submetrics for metric 'Sep 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '3.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '4.xlsx'.\n",
      "Missing submetrics for metric 'Oct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '4.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '5.xlsx'.\n",
      "Missing submetrics for metric 'Nov 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '5.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '6.xlsx'.\n",
      "Missing submetrics for metric 'Dec 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '6.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '7.xlsx'.\n",
      "Missing submetrics for metric 'Jan 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '7.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '8.xlsx'.\n",
      "Missing submetrics for metric 'Feb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '8.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '9.xlsx'.\n",
      "Missing submetrics for metric 'Mar 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '9.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '10.xlsx'.\n",
      "Missing submetrics for metric 'Apr 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '10.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '11.xlsx'.\n",
      "Missing submetrics for metric 'May 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '11.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '12.xlsx'.\n",
      "Missing submetrics for metric 'Jun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '12.xlsx' is empty. Skipping.\n",
      "Loaded weather data from '13.xlsx'.\n",
      "Missing submetrics for metric 'Jul 1 2 3 4 5 6 7 8 9 10 11 12 13 14' at row 1. Skipping.\n",
      "No valid weather data extracted.\n",
      "Reshaped weather data from '13.xlsx' is empty. Skipping.\n",
      "No valid weather data to combine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def reshape_weather_data(weather_df, year=2023, month=7):\n",
    "    \"\"\"\n",
    "    Reshapes the weather data from wide to long format, handling multi-row metrics.\n",
    "\n",
    "    Assumes that:\n",
    "    - The first row contains 'Time' and day identifiers (e.g., 'Jul 1', '2', '3', ..., '14')\n",
    "    - Each subsequent metric is followed by a single row containing submetrics and their values\n",
    "\n",
    "    Parameters:\n",
    "    - weather_df (pd.DataFrame): Raw weather data without headers.\n",
    "    - year (int): Year for the dates.\n",
    "    - month (int): Month number for the dates.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Reshaped weather data with one row per day and separate columns for each metric.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_metric = None\n",
    "    month_abbr = pd.to_datetime(f'{month}', format='%m').strftime('%b')\n",
    "    \n",
    "    # Extract day identifiers from the first row\n",
    "    days_row = weather_df.iloc[0, 1:].tolist()  # Skip first cell 'Time'\n",
    "    processed_days = []\n",
    "    for day in days_row:\n",
    "        if isinstance(day, str) and re.match(r'^[A-Za-z]+', day):\n",
    "            processed_days.append(day)\n",
    "        else:\n",
    "            processed_days.append(f\"{month_abbr} {day}\")\n",
    "    \n",
    "    # Iterate through the rest of the rows in pairs (metric and submetrics)\n",
    "    for idx in range(1, weather_df.shape[0], 2):\n",
    "        metric_row = weather_df.iloc[idx, :].tolist()\n",
    "        if not metric_row or pd.isnull(metric_row[0]):\n",
    "            continue  # Skip empty rows\n",
    "        \n",
    "        current_metric = metric_row[0]\n",
    "        if current_metric.lower() == 'precipitation (in)':\n",
    "            submetrics = ['Total']\n",
    "        else:\n",
    "            submetrics = ['Max', 'Avg', 'Min']\n",
    "        \n",
    "        # Next row contains submetrics and their values\n",
    "        if idx + 1 >= weather_df.shape[0]:\n",
    "            print(f\"Missing submetrics for metric '{current_metric}' at row {idx}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        submetric_row = weather_df.iloc[idx + 1, :].tolist()\n",
    "        if not submetric_row or pd.isnull(submetric_row[0]):\n",
    "            print(f\"Missing submetrics data for metric '{current_metric}' at row {idx + 1}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        submetrics_in_row = submetric_row[0].split()  # Split submetrics by space\n",
    "        if len(submetrics_in_row) != len(submetrics):\n",
    "            print(f\"Mismatch in submetrics for metric '{current_metric}'. Expected {len(submetrics)}, got {len(submetrics_in_row)}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract values for each submetric\n",
    "        values = submetric_row[1:]\n",
    "        expected_values = len(submetrics) * len(processed_days)\n",
    "        if len(values) < expected_values:\n",
    "            print(f\"Insufficient values for metric '{current_metric}'. Expected {expected_values}, got {len(values)}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        for day_idx, day in enumerate(processed_days):\n",
    "            for sub_idx, submetric in enumerate(submetrics):\n",
    "                value_idx = day_idx * len(submetrics) + sub_idx\n",
    "                if value_idx >= len(values):\n",
    "                    print(f\"Insufficient values for day '{day}' in metric '{current_metric}'.\")\n",
    "                    continue\n",
    "                value = values[value_idx]\n",
    "                \n",
    "                # Handle 'Total' submetric\n",
    "                if submetric.lower() == 'total':\n",
    "                    full_metric = 'Precipitation (in)'\n",
    "                else:\n",
    "                    full_metric = f\"{current_metric} ({submetric})\"\n",
    "                \n",
    "                # Parse the date\n",
    "                try:\n",
    "                    date = pd.to_datetime(f\"{day} {year}\", format='%b %d %Y', errors='coerce')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing date '{day} {year}': {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if pd.isnull(date):\n",
    "                    print(f\"Invalid date '{day} {year}'. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                data.append({\n",
    "                    'Date': date,\n",
    "                    'Metric': full_metric,\n",
    "                    'Value': value\n",
    "                })\n",
    "    \n",
    "    # Create a long-format DataFrame\n",
    "    weather_long_df = pd.DataFrame(data)\n",
    "    \n",
    "    if weather_long_df.empty:\n",
    "        print(\"No valid weather data extracted.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot to wide format\n",
    "    weather_pivot = weather_long_df.pivot_table(\n",
    "        index='Date',\n",
    "        columns='Metric',\n",
    "        values='Value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Convert metric columns to numeric types\n",
    "    for col in weather_pivot.columns:\n",
    "        if col != 'Date':\n",
    "            weather_pivot[col] = pd.to_numeric(weather_pivot[col], errors='coerce')\n",
    "    \n",
    "    return weather_pivot\n",
    "\n",
    "def preprocess_weather_files(weather_directory, output_csv='combined_weather_data.csv', num_files=13):\n",
    "    \"\"\"\n",
    "    Processes all weather Excel files, reshapes them, and combines into a single CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - weather_directory (str): Path to the directory containing weather Excel files.\n",
    "    - output_csv (str): Filename for the combined CSV.\n",
    "    - num_files (int): Number of Excel files to process.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    all_weather_dfs = []\n",
    "    start_month = 7  # Assuming file 1.xlsx is July\n",
    "    start_year = 2023  # Starting year\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_name = f\"{i}.xlsx\"\n",
    "        file_path = os.path.join(weather_directory, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Weather file '{file_name}' does not exist. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read the Excel file without headers\n",
    "            df_weather = pd.read_excel(file_path, header=None)\n",
    "            print(f\"Loaded weather data from '{file_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading '{file_name}': {e}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Determine the month and year\n",
    "        month = (start_month + i - 1) % 12 + 1\n",
    "        year = start_year + (start_month + i - 1) // 12\n",
    "        \n",
    "        # Reshape the weather data\n",
    "        reshaped_df = reshape_weather_data(df_weather, year=year, month=month)\n",
    "        \n",
    "        if not reshaped_df.empty:\n",
    "            all_weather_dfs.append(reshaped_df)\n",
    "            print(f\"Reshaped weather data from '{file_name}' with shape {reshaped_df.shape}.\")\n",
    "        else:\n",
    "            print(f\"Reshaped weather data from '{file_name}' is empty. Skipping.\")\n",
    "    \n",
    "    if all_weather_dfs:\n",
    "        combined_weather_df = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "        combined_weather_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Combined weather data saved to '{output_csv}' with shape {combined_weather_df.shape}.\")\n",
    "    else:\n",
    "        print(\"No valid weather data to combine.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directory containing your weather Excel files\n",
    "    weather_data_directory = 'ML-Proj-Dataset/Weather/'  # Replace with your actual weather data path\n",
    "    \n",
    "    # Preprocess and combine weather data\n",
    "    preprocess_weather_files(weather_data_directory, output_csv='combined_weather_data.csv', num_files=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598de25-0c23-4997-8e6c-3084b64d9993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6067b4-ee11-496f-b931-6e9d32588e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646e7d2-54a9-48dd-81d9-cf3425bf1fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55f78ef9-9679-4c4a-9c05-d1da61c7a8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of 'ML-Proj-Dataset/Weather/1.xlsx':\n",
      "                                                   0  \\\n",
      "0                                               Time   \n",
      "1  Jul 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ...   \n",
      "\n",
      "                                                   1  \\\n",
      "0                                   Temperature (°F)   \n",
      "1  Max Avg Min 97 88.8 81 99 91.5 82 102 91.7 77 ...   \n",
      "\n",
      "                                                   2  \\\n",
      "0                                     Dew Point (°F)   \n",
      "1  Max Avg Min 77 74.5 72 79 75.8 75 81 77.0 73 7...   \n",
      "\n",
      "                                                   3  \\\n",
      "0                                       Humidity (%)   \n",
      "1  Max Avg Min 74 63.4 47 79 61.3 47 100 63.7 45 ...   \n",
      "\n",
      "                                                   4  \\\n",
      "0                                   Wind Speed (mph)   \n",
      "1  Max Avg Min 14 8.9 5 12 6.7 0 35 11.0 0 32 13....   \n",
      "\n",
      "                                                   5  \\\n",
      "0                                      Pressure (in)   \n",
      "1  Max Avg Min 28.8 28.8 28.7 28.8 28.8 28.7 28.8...   \n",
      "\n",
      "                                                   6  \n",
      "0                                 Precipitation (in)  \n",
      "1  Total 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_weather_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and prints the first few rows of the given Excel weather file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, header=None)\n",
    "        print(f\"\\nContents of '{file_path}':\")\n",
    "        print(df.head(10))  # Print first 10 rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading '{file_path}': {e}\")\n",
    "\n",
    "\n",
    "weather_directory = 'ML-Proj-Dataset/Weather/'  # Replace with your actual path\n",
    "files = sorted([f for f in os.listdir(weather_directory) if f.endswith('.xlsx')])\n",
    "\n",
    "\n",
    "\n",
    "# Inspect the first file\n",
    "first_file = files[0]\n",
    "file_path = os.path.join(weather_directory, first_file)\n",
    "inspect_weather_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "342975a0-7861-4b93-ae77-9774c219c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df111 = pd.read_excel('ML-Proj-Dataset/Weather/1.xlsx', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2706ed14-c7af-4b6f-86d2-f9fdaa5dcef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       2 non-null      object\n",
      " 1   1       2 non-null      object\n",
      " 2   2       2 non-null      object\n",
      " 3   3       2 non-null      object\n",
      " 4   4       2 non-null      object\n",
      " 5   5       2 non-null      object\n",
      " 6   6       2 non-null      object\n",
      "dtypes: object(7)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df111.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24c635-035e-4301-8456-3ae32f837fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930a04d-61fa-4849-9738-fc9abe954431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd27119-47db-4314-beb5-0426a3d22b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
